{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "#os.environ[\"PYSPARK_PYTHON\"] = \"/opt/continuum/anaconda/bin/python\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/Library/Java/JavaVirtualMachines/jdk1.8.0_221.jdk/Contents/Home/jre\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/Users/lcx/package/spark-2.4.6-bin-hadoop2.7\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create a SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"twitter ML\").getOrCreate()\n",
    "        #.config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.11:2.6.3\")\\\n",
    "        #.config(\"spark.kryoserializer.buffer.max\", \"1000M\")\\\n",
    "        \n",
    "\n",
    "# get the spark context\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.25:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.6</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>twitter ML</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb2f385cf90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read data from asw folder, text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_path = \"../twitter_data/sample_data/2020_10_30/*/*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_schema = StructType([StructField(\"tweet_text\", StringType(), True), \n",
    "                          # StructField(\"hash_tag\", ArrayType(StringType(), True), True), \n",
    "                          StructField(\"hash_tag\", StringType(), True), \n",
    "                          StructField(\"created_at\", StringType(), True), \n",
    "                          StructField(\"retweet_count\", IntegerType(), True), \n",
    "                          StructField(\"favorite_count\", IntegerType(), True), \n",
    "                          StructField(\"retweeted\", BooleanType(), True), \n",
    "                          StructField(\"truncated\", BooleanType(), True), \n",
    "                          StructField(\"id\", StringType(), True), \n",
    "                          StructField(\"user_name\", StringType(), True), \n",
    "                          StructField(\"screen_name\", StringType(), True), \n",
    "                          StructField(\"followers_count\", IntegerType(), True), \n",
    "                          StructField(\"location\", StringType(), True), \n",
    "                          StructField(\"geo\", StringType(), True),\n",
    "                          StructField(\"invalid\", StringType(), True)])\n",
    "\n",
    "# file_schema = StructType([StructField(\"_c0\", StringType(), True), \n",
    "#                           StructField(\"_c1\", ArrayType(StringType(), True), True), \n",
    "#                           StructField(\"_c2\", StringType(), True), \n",
    "#                           StructField(\"_c3\", IntegerType(), True), \n",
    "#                           StructField(\"_c4\", IntegerType(), True), \n",
    "#                           StructField(\"_c5\", BooleanType(), True), \n",
    "#                           StructField(\"_c6\", BooleanType(), True), \n",
    "#                           StructField(\"_c7\", StringType(), True), \n",
    "#                           StructField(\"_c8\", StringType(), True), \n",
    "#                           StructField(\"_c9\", StringType(), True), \n",
    "#                           StructField(\"_c10\", IntegerType(), True), \n",
    "#                           StructField(\"_c11\", StringType(), True), \n",
    "#                           StructField(\"_c12\", StringType(), True),\n",
    "#                           StructField(\"_c13\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_needConversion',\n",
       " '_needSerializeAnyField',\n",
       " 'add',\n",
       " 'fieldNames',\n",
       " 'fields',\n",
       " 'fromInternal',\n",
       " 'fromJson',\n",
       " 'json',\n",
       " 'jsonValue',\n",
       " 'names',\n",
       " 'needConversion',\n",
       " 'simpleString',\n",
       " 'toInternal',\n",
       " 'typeName']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(file_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_schema.fieldNames())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(tweet_text,StringType,true),StructField(hash_tag,StringType,true),StructField(created_at,StringType,true),StructField(retweet_count,IntegerType,true),StructField(favorite_count,IntegerType,true),StructField(retweeted,BooleanType,true),StructField(truncated,BooleanType,true),StructField(id,StringType,true),StructField(user_name,StringType,true),StructField(screen_name,StringType,true),StructField(followers_count,IntegerType,true),StructField(location,StringType,true),StructField(geo,StringType,true),StructField(invalid,StringType,true)))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = (spark\n",
    "#       .read\n",
    "#       .csv(header=False, sep=\"\\t\", schema=file_schema, enforceSchema=True, \n",
    "#            path=files_path)) # don't know why this does not work\n",
    "\n",
    "df = (spark\n",
    "      .read\n",
    "      .format(\"csv\")\n",
    "      .options(header=False, sep=\"\\t\", enforceSchema=True)\n",
    "      .schema(file_schema)\n",
    "      .load(files_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-------------+--------------+---------+---------+-------------------+--------------------+---------------+---------------+--------------------+----+-------+\n",
      "|          tweet_text|            hash_tag|          created_at|retweet_count|favorite_count|retweeted|truncated|                 id|           user_name|    screen_name|followers_count|            location| geo|invalid|\n",
      "+--------------------+--------------------+--------------------+-------------+--------------+---------+---------+-------------------+--------------------+---------------+---------------+--------------------+----+-------+\n",
      "|.@mit_hst + @MITd...|Biology, COVID19,...|Fri Oct 30 20:19:...|            0|             0|    false|     true|1322271849028456448|                 HST|        mit_hst|           2449|       Cambridge, MA|None|   null|\n",
      "|Just when I am ab...|Past3amSquad, AI,...|Fri Oct 30 20:19:...|            0|             0|    false|     true|1322271849401778176|       Sydney K, PhD|sydneyfranklins|           1236|Pretoria, South A...|None|   null|\n",
      "|And a beneficial ...|     AI, FridayReads|Fri Oct 30 20:19:...|            0|             0|     true|    false|1322271852341784576|Beka \"Bexx\" Modebade|       bexxmodd|           5944|       Maryland, USA|None|   null|\n",
      "+--------------------+--------------------+--------------------+-------------+--------------+---------+---------+-------------------+--------------------+---------------+---------------+--------------------+----+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## write to parquet\n",
    "parquet_file_path = \"/Users/lcx/Documents/weclouddata/my_project/twitter_data/sample_data/2020_10_30_parquet\"\n",
    "(df.write.format(\"parquet\").mode(\"overwrite\").option(\"compression\", \"snappy\").save(parquet_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.sample(fraction=0.2, seed=1234)\n",
    "df_sample = df_sample.coalesce(1)\n",
    "\n",
    "parquet_file_path = \"/Users/lcx/Documents/weclouddata/my_project/twitter_data/sample_data/2020_10_30_subsample_parquet\"\n",
    "(df_sample.write.format(\"parquet\").mode(\"overwrite\").option(\"compression\", \"snappy\").save(parquet_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tweet_text: string (nullable = true)\n",
      " |-- hash_tag: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- retweet_count: integer (nullable = true)\n",
      " |-- favorite_count: integer (nullable = true)\n",
      " |-- retweeted: boolean (nullable = true)\n",
      " |-- truncated: boolean (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      " |-- screen_name: string (nullable = true)\n",
      " |-- followers_count: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- geo: string (nullable = true)\n",
      " |-- invalid: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(tweet_text='.@mit_hst + @MITdeptofBE prof. James Collins spoke about harnessing synthetic #Biology to develop diagnostics for #COVID19 + how his lab is using #DeepLearning  to enhance the design of such systems  @MITEECS #jclinic @MIT_IMES #MachineLearning https://t.co/2u3JwNj7CR https://t.co/EoAMQo7UHx', hash_tag='Biology, COVID19, DeepLearning, jclinic, MachineLearning', created_at='Fri Oct 30 20:19:10 +0000 2020', retweet_count=0, favorite_count=0, retweeted=False, truncated=True, id='1322271849028456448', user_name='HST', screen_name='mit_hst', followers_count=2449, location='Cambridge, MA', geo='None', invalid=None)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35463"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tweet_text', 'hash_tag', 'created_at', 'retweet_count', 'favorite_count', 'retweeted', 'truncated', 'id', 'user_name', 'screen_name', 'followers_count', 'location', 'geo', 'invalid']\n"
     ]
    }
   ],
   "source": [
    "print(file_schema.fieldNames())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns of interest\n",
    "cols_select = ['tweet_text', 'hash_tag', 'created_at', 'retweet_count', 'favorite_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-------------+--------------+\n",
      "|          tweet_text|            hash_tag|          created_at|retweet_count|favorite_count|\n",
      "+--------------------+--------------------+--------------------+-------------+--------------+\n",
      "|.@mit_hst + @MITd...|Biology, COVID19,...|Fri Oct 30 20:19:...|            0|             0|\n",
      "|Just when I am ab...|Past3amSquad, AI,...|Fri Oct 30 20:19:...|            0|             0|\n",
      "|And a beneficial ...|     AI, FridayReads|Fri Oct 30 20:19:...|            0|             0|\n",
      "|Check out new wor...|                  AI|Fri Oct 30 20:19:...|            0|             0|\n",
      "|South Korea 🇰🇷 ...|Healthcare, IoT, ...|Fri Oct 30 20:19:...|            0|             0|\n",
      "+--------------------+--------------------+--------------------+-------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_select = df.dropna(subset=[\"tweet_text\"]).select(cols_select)\n",
    "df_select.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35461"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_select.cache()\n",
    "df_select.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[tweet_text: string, hash_tag: string, created_at: string, retweet_count: int, favorite_count: int]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_select.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Basic cleaning\n",
    "1. remove mention @xxx, hashtag #xxx;\n",
    "2. remove hyperlink;\n",
    "3. remove special characters;\n",
    "4. remove any whitespace characters (change it to one)\n",
    "\n",
    "TODO: </br>\n",
    "* also fix the abbreviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_select_clean = (df_select.withColumn(\"tweet_text\", F.regexp_replace(\"tweet_text\", r\"[@#&][A-Za-z0-9_-]+\", \" \"))\n",
    "                   .withColumn(\"tweet_text\", F.regexp_replace(\"tweet_text\", r\"\\w+:\\/\\/\\S+\", \" \"))\n",
    "                   .withColumn(\"tweet_text\", F.regexp_replace(\"tweet_text\", r\"[^A-Za-z]\", \" \"))\n",
    "                   .withColumn(\"tweet_text\", F.regexp_replace(\"tweet_text\", r\"\\s+\", \" \"))\n",
    "                   .withColumn(\"tweet_text\", F.lower(F.col(\"tweet_text\")))\n",
    "                   .withColumn(\"tweet_text\", F.trim(F.col(\"tweet_text\")))\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-------------+--------------+\n",
      "|          tweet_text|            hash_tag|          created_at|retweet_count|favorite_count|\n",
      "+--------------------+--------------------+--------------------+-------------+--------------+\n",
      "|prof james collin...|Biology, COVID19,...|Fri Oct 30 20:19:...|            0|             0|\n",
      "|just when i am ab...|Past3amSquad, AI,...|Fri Oct 30 20:19:...|            0|             0|\n",
      "|and a beneficial ...|     AI, FridayReads|Fri Oct 30 20:19:...|            0|             0|\n",
      "|check out new wor...|                  AI|Fri Oct 30 20:19:...|            0|             0|\n",
      "|south korea bus s...|Healthcare, IoT, ...|Fri Oct 30 20:19:...|            0|             0|\n",
      "+--------------------+--------------------+--------------------+-------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_select_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[tweet_text: string, hash_tag: string, created_at: string, retweet_count: int, favorite_count: int]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_select_clean.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".@mit_hst + @MITdeptofBE prof. James Collins spoke about harnessing synthetic #Biology to develop diagnostics for #COVID19 + how his lab is using #DeepLearning  to enhance the design of such systems  @MITEECS #jclinic @MIT_IMES #MachineLearning https://t.co/2u3JwNj7CR https://t.co/EoAMQo7UHx \n",
      "\n",
      "Just when I am about to have a glass of Gin and lemonade.... #Past3amSquad #AI #MachineLearning #DeepLearning #Coding https://t.co/sAOiSemArA \n",
      "\n",
      "And a beneficial use of #AI: Using Artificial Intelligence to Identify Powerful New Antibiotic #FridayReads  https://t.co/K8mPDdPShd \n",
      "\n",
      "Check out new work on my @Behance profile: \"Community Bank / CREDIT CARD DESIGN\" https://t.co/xAF0gSaniv   #AI \n",
      "\n",
      "South Korea 🇰🇷 bus 🚌 stops check your temperature 🤒 to fight coronavirus 😷 @wef   #Healthcare #IoT #AI #DigitalTransformation #Technology #Innovation #HealthTech via @automeme #SmartCity #COVID19 cc @mvollmer1 @IrmaRaste @Ronald_vanLoon @ipfconline1  https://t.co/YyW9hmONVX \n",
      "\n",
      "ARRIA is the language of data &amp; voice reports #freetrial #nlg Reports from #excel #powerbi #alexa #qlik #resultsbi #uipath #tibco #tableau #cpa #ai #microstrategy #covid19 Join the #bi LEADERS LOVE always defeats FEAR https://t.co/szpOjPqLc8 https://t.co/X8EGwdUTjs \n",
      "\n",
      "The latest Thinking Different Daily! https://t.co/btApka9sUD Thanks to @aneeman #ai #bropenscience \n",
      "\n",
      "Thank you @_KarenHao for featuring our work in @techreview Beautifully written! Fourier neural operator gets 1000x speedup on Navier Stokes PDEs As a bonus @_KarenHao also talks about @MCHammer time :) @Caltech @kazizzad @ZongyiLiCaltech #AI #DeepLearning https://t.co/ajNyUqy58e \n",
      "\n",
      "REMINDER: Massachusetts Paid Family and Medical Leave Next Steps https://t.co/4o3equvXpj #TheLegalLowdown #VOTE #LawSuits #LawSuit #Court #Lawyer #Litigation #Lawfirm #Justice #CaseLaw #IP #SCOTUS #CorporateLaw #COVID19 #CyberLaw #Security #TechNews #AI #WhiteCase #BakerMcKenzie \n",
      "\n",
      "Labor &amp; Employment E-Note - October 2020 https://t.co/wAFrp0VfNp #TheLegalLowdown #VOTE #LawSuits #LawSuit #Court #Lawyer #Litigation #Lawfirm #Justice #CaseLaw #IP #SCOTUS #CorporateLaw #COVID19 #CyberLaw #Security #TechNews #AI #WhiteCase #BakerMcKenzie \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sample_pd_unclean = df_select.limit(10).toPandas()\n",
    "\n",
    "for i in range(10):\n",
    "    print(df_sample_pd_unclean.iloc[i, 0], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_pd_clean = df_select_clean.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prof james collins spoke about harnessing synthetic to develop diagnostics for how his lab is using to enhance the design of such systems \n",
      "\n",
      "just when i am about to have a glass of gin and lemonade \n",
      "\n",
      "and a beneficial use of using artificial intelligence to identify powerful new antibiotic \n",
      "\n",
      "check out new work on my profile community bank credit card design \n",
      "\n",
      "south korea bus stops check your temperature to fight coronavirus via cc \n",
      "\n",
      "arria is the language of data voice reports reports from join the leaders love always defeats fear \n",
      "\n",
      "the latest thinking different daily thanks to \n",
      "\n",
      "thank you for featuring our work in beautifully written fourier neural operator gets 1000x speedup on navier stokes pdes as a bonus also talks about time \n",
      "\n",
      "reminder massachusetts paid family and medical leave next steps \n",
      "\n",
      "labor employment e note october 2020 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(df_sample_pd_clean.iloc[i, 0], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35461"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_select_clean.dropna(subset=[\"tweet_text\"]).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenzier = Tokenizer(inputCol=\"tweet_text\", outputCol=\"tokens\")\n",
    "\n",
    "df_tokenized = tokenzier.transform(df_select_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-------------+--------------+--------------------+\n",
      "|          tweet_text|            hash_tag|          created_at|retweet_count|favorite_count|              tokens|\n",
      "+--------------------+--------------------+--------------------+-------------+--------------+--------------------+\n",
      "|prof james collin...|Biology, COVID19,...|Fri Oct 30 20:19:...|            0|             0|[prof, james, col...|\n",
      "|just when i am ab...|Past3amSquad, AI,...|Fri Oct 30 20:19:...|            0|             0|[just, when, i, a...|\n",
      "|and a beneficial ...|     AI, FridayReads|Fri Oct 30 20:19:...|            0|             0|[and, a, benefici...|\n",
      "|check out new wor...|                  AI|Fri Oct 30 20:19:...|            0|             0|[check, out, new,...|\n",
      "|south korea bus s...|Healthcare, IoT, ...|Fri Oct 30 20:19:...|            0|             0|[south, korea, bu...|\n",
      "+--------------------+--------------------+--------------------+-------------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tokenized.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**some quick analyses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-------------+--------------+--------------------+----------+\n",
      "|          tweet_text|            hash_tag|          created_at|retweet_count|favorite_count|              tokens|wordCounts|\n",
      "+--------------------+--------------------+--------------------+-------------+--------------+--------------------+----------+\n",
      "|prof james collin...|Biology, COVID19,...|Fri Oct 30 20:19:...|            0|             0|[prof, james, col...|        23|\n",
      "|just when i am ab...|Past3amSquad, AI,...|Fri Oct 30 20:19:...|            0|             0|[just, when, i, a...|        13|\n",
      "|and a beneficial ...|     AI, FridayReads|Fri Oct 30 20:19:...|            0|             0|[and, a, benefici...|        13|\n",
      "|check out new wor...|                  AI|Fri Oct 30 20:19:...|            0|             0|[check, out, new,...|        12|\n",
      "|south korea bus s...|Healthcare, IoT, ...|Fri Oct 30 20:19:...|            0|             0|[south, korea, bu...|        12|\n",
      "+--------------------+--------------------+--------------------+-------------+--------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_select_clean_analysis = df_tokenized.withColumn(\"wordCounts\", F.size(F.col(\"tokens\")))\n",
    "df_select_clean_analysis.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+-------------------+--------------+-----------------+------------------+--------------+-----------------+------------------+\n",
      "|  wordCounts_mean|retweet_count_mean|favorite_count_mean|wordCounts_min|retweet_count_min|favorite_count_min|wordCounts_max|retweet_count_max|favorite_count_max|\n",
      "+-----------------+------------------+-------------------+--------------+-----------------+------------------+--------------+-----------------+------------------+\n",
      "|14.99416260116748|               0.0|                0.0|             1|                0|                 0|            61|                0|                 0|\n",
      "+-----------------+------------------+-------------------+--------------+-----------------+------------------+--------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agg_col = [\"wordCounts\", \"retweet_count\", \"favorite_count\"]\n",
    "# [F.min(F.col(c)).alias(c+\"_min\") for c in agg_col]\n",
    "df_select_clean_analysis.agg(*[F.mean(F.col(c)).alias(c+\"_mean\") for c in agg_col],\n",
    "                             *[F.min(F.col(c)).alias(c+\"_min\") for c in agg_col], \n",
    "                             *[F.max(F.col(c)).alias(c+\"_max\") for c in agg_col]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------+---------------+\n",
      "|  avg(wordCounts)|min(wordCounts)|max(wordCounts)|\n",
      "+-----------------+---------------+---------------+\n",
      "|14.99416260116748|              1|             61|\n",
      "+-----------------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_select_clean_analysis.select(F.avg(F.col(\"wordCounts\")), \n",
    "                                F.min(F.col(\"wordCounts\")), \n",
    "                                F.max(F.col(\"wordCounts\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8.0, 13.0, 20.0]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_select_clean_analysis.approxQuantile(col=\"wordCounts\", \n",
    "                                        probabilities=[0.25, 0.5, 0.75], \n",
    "                                        relativeError=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Remove stop words\n",
    "TODO: extend stop words </br>\n",
    "(\"also\", \"via\", \"cc\", \"rt\", \"must\", \"always\"...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"remove_stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your']\n"
     ]
    }
   ],
   "source": [
    "stopwords_list = stopword_remover.getStopWords()\n",
    "print(len(stopwords_list))\n",
    "print(stopwords_list[:10])\n",
    "# print(sorted(stopword_remover.getStopWords()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"where's\", \"who's\", \"why's\", 'would', 'also', 'via', 'cc', 'rt', 'must', 'always']\n"
     ]
    }
   ],
   "source": [
    "more_stopwords = [\"also\", \"via\", \"cc\", \"rt\", \"must\", \"always\"]\n",
    "stopwords_list = stopwords_list + more_stopwords\n",
    "print(stopwords_list[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', \"i'll\", \"you'll\", \"he'll\", \"she'll\", \"we'll\", \"they'll\", \"i'd\", \"you'd\", \"he'd\", \"she'd\", \"we'd\", \"they'd\", \"i'm\", \"you're\", \"he's\", \"she's\", \"it's\", \"we're\", \"they're\", \"i've\", \"we've\", \"you've\", \"they've\", \"isn't\", \"aren't\", \"wasn't\", \"weren't\", \"haven't\", \"hasn't\", \"hadn't\", \"don't\", \"doesn't\", \"didn't\", \"won't\", \"wouldn't\", \"shan't\", \"shouldn't\", \"mustn't\", \"can't\", \"couldn't\", 'cannot', 'could', \"here's\", \"how's\", \"let's\", 'ought', \"that's\", \"there's\", \"what's\", \"when's\", \"where's\", \"who's\", \"why's\", 'would', 'also', 'via', 'cc', 'rt', 'must', 'always']\n"
     ]
    }
   ],
   "source": [
    "stopword_remover.setStopWords(stopwords_list)\n",
    "print(stopword_remover.getStopWords())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rmstop = stopword_remover.transform(df_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-------------+--------------+--------------------+--------------------+\n",
      "|          tweet_text|            hash_tag|          created_at|retweet_count|favorite_count|              tokens|         remove_stop|\n",
      "+--------------------+--------------------+--------------------+-------------+--------------+--------------------+--------------------+\n",
      "|prof james collin...|Biology, COVID19,...|Fri Oct 30 20:19:...|            0|             0|[prof, james, col...|[prof, james, col...|\n",
      "|just when i am ab...|Past3amSquad, AI,...|Fri Oct 30 20:19:...|            0|             0|[just, when, i, a...|[glass, gin, lemo...|\n",
      "|and a beneficial ...|     AI, FridayReads|Fri Oct 30 20:19:...|            0|             0|[and, a, benefici...|[beneficial, use,...|\n",
      "|check out new wor...|                  AI|Fri Oct 30 20:19:...|            0|             0|[check, out, new,...|[check, new, work...|\n",
      "|south korea bus s...|Healthcare, IoT, ...|Fri Oct 30 20:19:...|            0|             0|[south, korea, bu...|[south, korea, bu...|\n",
      "+--------------------+--------------------+--------------------+-------------+--------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rmstop.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['prof', 'james', 'collins', 'spoke', 'about', 'harnessing', 'synthetic', 'to', 'develop', 'diagnostics', 'for', 'how', 'his', 'lab', 'is', 'using', 'to', 'enhance', 'the', 'design', 'of', 'such', 'systems'] \n",
      "\n",
      "['prof', 'james', 'collins', 'spoke', 'harnessing', 'synthetic', 'develop', 'diagnostics', 'lab', 'using', 'enhance', 'design', 'systems'] \n",
      "\n",
      "['just', 'when', 'i', 'am', 'about', 'to', 'have', 'a', 'glass', 'of', 'gin', 'and', 'lemonade'] \n",
      "\n",
      "['glass', 'gin', 'lemonade'] \n",
      "\n",
      "['and', 'a', 'beneficial', 'use', 'of', 'using', 'artificial', 'intelligence', 'to', 'identify', 'powerful', 'new', 'antibiotic'] \n",
      "\n",
      "['beneficial', 'use', 'using', 'artificial', 'intelligence', 'identify', 'powerful', 'new', 'antibiotic'] \n",
      "\n",
      "['check', 'out', 'new', 'work', 'on', 'my', 'profile', 'community', 'bank', 'credit', 'card', 'design'] \n",
      "\n",
      "['check', 'new', 'work', 'profile', 'community', 'bank', 'credit', 'card', 'design'] \n",
      "\n",
      "['south', 'korea', 'bus', 'stops', 'check', 'your', 'temperature', 'to', 'fight', 'coronavirus', 'via', 'cc'] \n",
      "\n",
      "['south', 'korea', 'bus', 'stops', 'check', 'temperature', 'fight', 'coronavirus', 'via', 'cc'] \n",
      "\n",
      "['arria', 'is', 'the', 'language', 'of', 'data', 'voice', 'reports', 'reports', 'from', 'join', 'the', 'leaders', 'love', 'always', 'defeats', 'fear'] \n",
      "\n",
      "['arria', 'language', 'data', 'voice', 'reports', 'reports', 'join', 'leaders', 'love', 'always', 'defeats', 'fear'] \n",
      "\n",
      "['the', 'latest', 'thinking', 'different', 'daily', 'thanks', 'to'] \n",
      "\n",
      "['latest', 'thinking', 'different', 'daily', 'thanks'] \n",
      "\n",
      "['thank', 'you', 'for', 'featuring', 'our', 'work', 'in', 'beautifully', 'written', 'fourier', 'neural', 'operator', 'gets', '1000x', 'speedup', 'on', 'navier', 'stokes', 'pdes', 'as', 'a', 'bonus', 'also', 'talks', 'about', 'time'] \n",
      "\n",
      "['thank', 'featuring', 'work', 'beautifully', 'written', 'fourier', 'neural', 'operator', 'gets', '1000x', 'speedup', 'navier', 'stokes', 'pdes', 'bonus', 'also', 'talks', 'time'] \n",
      "\n",
      "['reminder', 'massachusetts', 'paid', 'family', 'and', 'medical', 'leave', 'next', 'steps'] \n",
      "\n",
      "['reminder', 'massachusetts', 'paid', 'family', 'medical', 'leave', 'next', 'steps'] \n",
      "\n",
      "['labor', 'employment', 'e', 'note', 'october', '2020'] \n",
      "\n",
      "['labor', 'employment', 'e', 'note', 'october', '2020'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df_sample_pd_before = df_tokenized.limit(10).toPandas()\n",
    "df_sample_pd_rmstop = df_rmstop.limit(10).toPandas()\n",
    "\n",
    "for i in range(10):\n",
    "    print(df_sample_pd_rmstop.loc[i, \"tokens\"], \"\\n\")\n",
    "    print(df_sample_pd_rmstop.loc[i, \"remove_stop\"], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4. Lemmatization\n",
    "**Note:** </br>\n",
    "1. here we use the stemming for fast processing;\n",
    "2. One may use the lemmatization in future. Note that one should define a parts-of-speech to obtain the correct lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tweet_text: string (nullable = true)\n",
      " |-- hash_tag: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- retweet_count: integer (nullable = true)\n",
      " |-- favorite_count: integer (nullable = true)\n",
      " |-- tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- remove_stop: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rmstop.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stemmer library\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate stemmer object\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**so pandas_udf cannot handle complex data?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(words_list):\n",
    "    return [stemmer.stem(word) for word in words_list]\n",
    "\n",
    "stem_udf = F.udf(stem, returnType = ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stemmed = df_rmstop.select(\"remove_stop\", stem_udf(F.col(\"remove_stop\")).alias(\"stemmed\")).limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['prof', 'james', 'collins', 'spoke', 'harnessing', 'synthetic', 'develop', 'diagnostics', 'lab', 'using', 'enhance', 'design', 'systems'] \n",
      "\n",
      "['prof', 'jame', 'collin', 'spoke', 'har', 'synthet', 'develop', 'diagnost', 'lab', 'use', 'enhanc', 'design', 'system'] \n",
      "\n",
      "['glass', 'gin', 'lemonade'] \n",
      "\n",
      "['glass', 'gin', 'lemonad'] \n",
      "\n",
      "['beneficial', 'use', 'using', 'artificial', 'intelligence', 'identify', 'powerful', 'new', 'antibiotic'] \n",
      "\n",
      "['benefici', 'use', 'use', 'artifici', 'intellig', 'identifi', 'power', 'new', 'antibiot'] \n",
      "\n",
      "['check', 'new', 'work', 'profile', 'community', 'bank', 'credit', 'card', 'design'] \n",
      "\n",
      "['check', 'new', 'work', 'profil', 'commun', 'bank', 'credit', 'card', 'design'] \n",
      "\n",
      "['south', 'korea', 'bus', 'stops', 'check', 'temperature', 'fight', 'coronavirus'] \n",
      "\n",
      "['south', 'korea', 'bu', 'stop', 'check', 'temperatur', 'fight', 'coronaviru'] \n",
      "\n",
      "['arria', 'language', 'data', 'voice', 'reports', 'reports', 'join', 'leaders', 'love', 'defeats', 'fear'] \n",
      "\n",
      "['arria', 'languag', 'data', 'voic', 'report', 'report', 'join', 'leader', 'love', 'defeat', 'fear'] \n",
      "\n",
      "['latest', 'thinking', 'different', 'daily', 'thanks'] \n",
      "\n",
      "['latest', 'think', 'differ', 'daili', 'thank'] \n",
      "\n",
      "['thank', 'featuring', 'work', 'beautifully', 'written', 'fourier', 'neural', 'operator', 'gets', 'x', 'speedup', 'navier', 'stokes', 'pdes', 'bonus', 'talks', 'time'] \n",
      "\n",
      "['thank', 'featur', 'work', 'beauti', 'written', 'fourier', 'neural', 'oper', 'get', 'x', 'speedup', 'navier', 'stoke', 'pde', 'bonu', 'talk', 'time'] \n",
      "\n",
      "['reminder', 'massachusetts', 'paid', 'family', 'medical', 'leave', 'next', 'steps'] \n",
      "\n",
      "['remind', 'massachusett', 'paid', 'famili', 'medic', 'leav', 'next', 'step'] \n",
      "\n",
      "['labor', 'employment', 'e', 'note', 'october'] \n",
      "\n",
      "['labor', 'employ', 'e', 'note', 'octob'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(df_stemmed.loc[i, \"remove_stop\"], \"\\n\")\n",
    "    print(df_stemmed.loc[i, \"stemmed\"], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-------------+--------------+--------------------+--------------------+--------------------+\n",
      "|          tweet_text|            hash_tag|          created_at|retweet_count|favorite_count|              tokens|         remove_stop|             stemmed|\n",
      "+--------------------+--------------------+--------------------+-------------+--------------+--------------------+--------------------+--------------------+\n",
      "|prof james collin...|Biology, COVID19,...|Fri Oct 30 20:19:...|            0|             0|[prof, james, col...|[prof, james, col...|[prof, jame, coll...|\n",
      "|just when i am ab...|Past3amSquad, AI,...|Fri Oct 30 20:19:...|            0|             0|[just, when, i, a...|[glass, gin, lemo...|[glass, gin, lemo...|\n",
      "|and a beneficial ...|     AI, FridayReads|Fri Oct 30 20:19:...|            0|             0|[and, a, benefici...|[beneficial, use,...|[benefici, use, u...|\n",
      "|check out new wor...|                  AI|Fri Oct 30 20:19:...|            0|             0|[check, out, new,...|[check, new, work...|[check, new, work...|\n",
      "|south korea bus s...|Healthcare, IoT, ...|Fri Oct 30 20:19:...|            0|             0|[south, korea, bu...|[south, korea, bu...|[south, korea, bu...|\n",
      "+--------------------+--------------------+--------------------+-------------+--------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_stemmed = df_rmstop.withColumn(\"stemmed\", stem_udf(F.col(\"remove_stop\")))\n",
    "\n",
    "df_stemmed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35461"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stemmed.cache()\n",
    "df_stemmed.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5. More to consider\n",
    "1. use bigram\n",
    "2. filter short vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6. Encode words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.6.1. CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should tune some parameter\n",
    "vectorizer = CountVectorizer(inputCol= \"stemmed\", outputCol=\"rawFeatures\")\n",
    "vectorizer_model = vectorizer.fit(df_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vect = vectorizer_model.transform(df_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+\n",
      "|stemmed                                                                                       |rawFeatures                                                                                                          |\n",
      "+----------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+\n",
      "|[prof, jame, collin, spoke, har, synthet, develop, diagnost, lab, use, enhanc, design, system]|(11147,[3,17,36,123,362,651,928,1157,1310,1621,1638,2268,4047],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|[glass, gin, lemonad]                                                                         |(11147,[2311,6242,6464],[1.0,1.0,1.0])                                                                               |\n",
      "|[benefici, use, use, artifici, intellig, identifi, power, new, antibiot]                      |(11147,[3,4,6,7,51,181,3267,6458],[2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                 |\n",
      "|[check, new, work, profil, commun, bank, credit, card, design]                                |(11147,[7,14,47,123,245,427,1052,1213,1837],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                   |\n",
      "|[south, korea, bu, stop, check, temperatur, fight, coronaviru]                                |(11147,[47,192,356,868,1361,1508,1677,1792],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                       |\n",
      "+----------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_vect.select(\"stemmed\", \"rawFeatures\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.6.2. TF-IDF\n",
    "**do we need IDF for the LDA model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idf_model = idf.fit(df_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idf = idf_model.transform(df_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|rawFeatures                                                                                                          |features                                                                                                                                                                                                                                                                                                   |\n",
      "+---------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|(11147,[3,17,36,123,362,651,928,1157,1310,1621,1638,2268,4047],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|(11147,[3,17,36,123,362,651,928,1157,1310,1621,1638,2268,4047],[2.540629594027734,3.528279911001965,3.741625319643986,4.440735547092178,5.272210292540138,5.8610964627756745,6.256709274440827,6.605015968709043,6.715016863923371,7.010481076817207,7.042229775131788,7.531778000450493,8.53030683056162])|\n",
      "|(11147,[2311,6242,6464],[1.0,1.0,1.0])                                                                               |(11147,[2311,6242,6464],[7.585845221720769,9.089922618497043,9.089922618497043])                                                                                                                                                                                                                           |\n",
      "|(11147,[3,4,6,7,51,181,3267,6458],[2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                 |(11147,[3,4,6,7,51,181,3267,6458],[5.081259188055468,2.511713616065386,2.634724055156921,2.9457369843713974,3.92943156885641,4.717315205739653,8.173631886622887,9.089922618497043])                                                                                                                       |\n",
      "|(11147,[7,14,47,123,245,427,1052,1213,1837],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                   |(11147,[7,14,47,123,245,427,1052,1213,1837],[2.9457369843713974,3.4832018566839733,3.8402704239302876,4.440735547092178,4.954756061754687,5.556236053788809,6.415773969070514,6.692027345698673,7.257341154748733])                                                                                        |\n",
      "|(11147,[47,192,356,868,1361,1508,1677,1792],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                       |(11147,[47,192,356,868,1361,1508,1677,1792],[3.8402704239302876,4.7657899622420645,5.261281222007948,6.185757538468542,6.762644912912626,6.892698041160823,7.042229775131788,7.180380113612605])                                                                                                           |\n",
      "+---------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_idf.select(\"rawFeatures\", \"features\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7. Fit a LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA(k=10, seed=1234, optimizer=\"em\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_mode = lda.fit(df_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.clustering.DistributedLDAModel"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lda_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|topic|termIndices                                  |termWeights                                                                                                                                                                                                                    |\n",
      "+-----+---------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0    |[59, 48, 29, 36, 25, 53, 222, 109, 134, 215] |[0.011947539463108494, 0.01063126480549928, 0.010105796113778636, 0.008882373253693154, 0.00837318339170578, 0.007948565397035375, 0.0074253265998271475, 0.007265663175136275, 0.0062524635393256985, 0.00588747633074689]    |\n",
      "|1    |[11, 28, 75, 74, 71, 10, 60, 276, 3, 6]      |[0.010844650582157339, 0.0070469084452394375, 0.006928830957186309, 0.006866197636927214, 0.006208993840309192, 0.005621619598212959, 0.005609649787727194, 0.0055083147163164405, 0.0051914029396377655, 0.005156815200744344]|\n",
      "|2    |[0, 5, 100, 126, 138, 279, 26, 122, 243, 330]|[0.007290441029194178, 0.006026177408256685, 0.005733918893081006, 0.00564167953953037, 0.005545544223738591, 0.005130653646922274, 0.0050435431890586396, 0.004672728213931804, 0.004658262881721921, 0.004628697656290367]   |\n",
      "|3    |[43, 233, 81, 99, 136, 37, 168, 185, 26, 158]|[0.014132647733513134, 0.007998013234282734, 0.007353920390301814, 0.00722121151587412, 0.007011248995826723, 0.006751863027970654, 0.006697212352841304, 0.006548418858426025, 0.006473939414946778, 0.0063490077030719915]   |\n",
      "|4    |[0, 91, 201, 23, 5, 240, 18, 154, 257, 256]  |[0.009778080750940584, 0.00942256371120872, 0.008349500835985235, 0.00823144923958763, 0.0076138629901885725, 0.007558819004691664, 0.007345371244264397, 0.00723078937978822, 0.007192614451330378, 0.007125852288350366]     |\n",
      "|5    |[0, 1, 23, 142, 21, 96, 33, 16, 55, 5]       |[0.009158971569591882, 0.008775866757553484, 0.008105727444985621, 0.007566233505720237, 0.007174410337058596, 0.007152033188860657, 0.0065001863096161225, 0.006144416296737962, 0.006123201163407048, 0.006029619794804934]  |\n",
      "|6    |[0, 39, 15, 5, 6, 4, 2, 7, 80, 287]          |[0.011303862533418927, 0.008401524666283943, 0.00802918182207962, 0.007502398913212874, 0.006993771649272774, 0.006092639058103801, 0.005670317149377339, 0.0055221343628456595, 0.005239284549757711, 0.005058663404985317]   |\n",
      "|7    |[1, 35, 16, 220, 17, 44, 394, 0, 404, 88]    |[0.010354985595794937, 0.007111829529944265, 0.00630115610146014, 0.006082891788139537, 0.005381246434905856, 0.005297062106582164, 0.004996880025656844, 0.004798030942892067, 0.004788809656236619, 0.004788154751826607]    |\n",
      "|8    |[157, 63, 40, 9, 69, 3, 114, 204, 4, 250]    |[0.009420963354283681, 0.009176728158457869, 0.00863816361626158, 0.00856703214483069, 0.008095848499506689, 0.008044555920889634, 0.007952659645304954, 0.007876770991927535, 0.007595969406629533, 0.006860173434909104]     |\n",
      "|9    |[2, 15, 112, 0, 113, 68, 150, 6, 8, 1]       |[0.011765631057908763, 0.00718678532816101, 0.006592088619788698, 0.00655324572533005, 0.005982961289575343, 0.005976177454996804, 0.0059048333033298785, 0.005866257224465143, 0.00540805178458731, 0.005309671612768222]     |\n",
      "+-----+---------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topics_describe = lda_mode.describeTopics()\n",
    "topics_describe.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.7.1. convert the word vector to vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262144"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_model.getVocabSize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['learn',\n",
       " 'ai',\n",
       " 'data',\n",
       " 'use',\n",
       " 'intellig',\n",
       " 'machin',\n",
       " 'artifici',\n",
       " 'new',\n",
       " 'read',\n",
       " 'help']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_model.vocabulary[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def termsIdx2Term(vocabulary):\n",
    "    def termsIdx2Term(termIndices):\n",
    "        return [vocabulary[int(index)] for index in termIndices]\n",
    "    return F.udf(termsIdx2Term, ArrayType(StringType()))\n",
    "\n",
    "vocabList = vectorizer_model.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = topics_describe.withColumn(\"Terms\", termsIdx2Term(vocabList)(\"termIndices\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------+\n",
      "|Terms                                                                           |\n",
      "+--------------------------------------------------------------------------------+\n",
      "|[digit, process, compani, system, free, report, trust, secur, video, play]      |\n",
      "|[technolog, time, global, code, day, busi, program, tutori, use, artifici]      |\n",
      "|[learn, machin, custom, cloud, success, integr, like, insight, pm, cool]        |\n",
      "|[take, iot, innov, talk, watch, see, patient, healthcar, like, decis]           |\n",
      "|[learn, covid, record, best, machin, cough, model, avail, asymptomat, pleas]    |\n",
      "|[learn, ai, best, trend, make, platform, build, get, start, machin]             |\n",
      "|[learn, research, scienc, machin, artifici, intellig, data, new, network, visit]|\n",
      "|[ai, predict, get, billion, develop, know, built, learn, mobil, onlin]          |\n",
      "|[valu, analyt, gener, help, product, use, smart, across, intellig, boost]       |\n",
      "|[data, scienc, real, learn, manag, find, becom, artifici, read, ai]             |\n",
      "+--------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final.select(\"Terms\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.7.2. check the model performance\n",
    "**Note:** the calculation of log Perplexity requires a large topicsMatrix() to the driver. So one should use subsample..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.142064829812528"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_mode.logPerplexity(df_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_check_score = df_idf.sample(fraction=0.2, seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.7477518265845"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_mode.logPerplexity(df_sample_check_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|tweet_text                                                                                    |topicDistribution                                                                                                                                                                                        |\n",
      "+----------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|the latest thinking different daily thanks to                                                 |[0.09336869368277642,0.09681352250095995,0.11344034364828275,0.10397684408848612,0.10290955808190527,0.08637613665921207,0.10619130705128577,0.09171519953632526,0.09031063801288917,0.11489775673787742]|\n",
      "|weekly covid oversight enforcement report october                                             |[0.1621256442641239,0.07032450769373433,0.06715798266581997,0.06660924274532047,0.12064729145674175,0.06856954200120507,0.1823753079751231,0.11959137214702567,0.07634403020136973,0.06625507884953585]  |\n",
      "|a pytorch re implementation of gpt training                                                   |[0.0755009805725918,0.08948835077413987,0.1873849725379424,0.09091353849688717,0.08460948105467761,0.08240920137918302,0.07731092828018263,0.08777764232974977,0.14683177802834446,0.07777312654630127]  |\n",
      "|beyond the hype artificial intelligence in cybersecurity us chamber of commerce read more here|[0.06360133525585714,0.31425085132361746,0.06616193148599803,0.07884785587132541,0.07493949477723232,0.08508085620149582,0.07766732396873993,0.06145126024748134,0.1074607381806393,0.07053835268761334] |\n",
      "|untitled pieces from daicon series                                                            |[0.28822454225566635,0.10308445807112745,0.07001329715108195,0.06900075438131877,0.06809111084500977,0.09622908199465634,0.0680547811695838,0.0691370035233272,0.09985897485109949,0.06830599575712874]  |\n",
      "+----------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda_mode.transform(df_sample_check_score).select(\"tweet_text\", \n",
    "                                                 \"topicDistribution\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12407824.177683208"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_mode.trainingLogLikelihood()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.7.3. let's try not using IDF (bad idea)\n",
    "**Note:** Very bad. The topics will be dominated by those frequent words, machine learning, artificial intellegence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_tf = LDA(k=10, seed=1234, optimizer=\"em\", featuresCol=\"rawFeatures\")\n",
    "\n",
    "lda_tf_model = lda_tf.fit(df_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|topic|termIndices                    |termWeights                                                                                                                                                                                                                 |\n",
      "+-----+-------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0    |[0, 1, 2, 4, 3, 5, 6, 7, 8, 10]|[0.016355966530646766, 0.01368392013664132, 0.01078462281003215, 0.009592814521427417, 0.009523470500275499, 0.00897987390714028, 0.00833056408119102, 0.006137820622040562, 0.005657180654433457, 0.004724361876742398]    |\n",
      "|1    |[0, 1, 2, 4, 3, 5, 6, 7, 8, 9] |[0.01674125987674094, 0.013836033989361183, 0.010514667669998734, 0.009619098676826409, 0.00951057884083387, 0.009328430654643768, 0.00819583609882383, 0.006351807615409523, 0.005852062443836558, 0.004796396794345415]   |\n",
      "|2    |[0, 1, 2, 4, 5, 3, 6, 7, 8, 9] |[0.016678396647643444, 0.013848723773398429, 0.010363385036167867, 0.009473958227851347, 0.00916455406456878, 0.009096299137194222, 0.008456342781593225, 0.0063272611022692, 0.005629927235439561, 0.004542910133624734]   |\n",
      "|3    |[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] |[0.016507576030613813, 0.013740169161258975, 0.010079486285704384, 0.009297274744872158, 0.00920358460659004, 0.008968254281231961, 0.008062667460684101, 0.006156234115448774, 0.005536743068726564, 0.004565252305081276] |\n",
      "|4    |[0, 1, 2, 3, 5, 4, 6, 7, 8, 9] |[0.016891467122044545, 0.013634400073316212, 0.010530332908842527, 0.009512557055175478, 0.00922158919162899, 0.009007593112910689, 0.007813231730684713, 0.00614876236421118, 0.005630327428865948, 0.004721288594529405]  |\n",
      "|5    |[0, 1, 2, 3, 5, 4, 6, 7, 8, 9] |[0.016625693650282207, 0.014129536588727825, 0.010649868720693272, 0.009512060440073574, 0.009491257099844028, 0.00942375093504827, 0.00840730612134764, 0.006355126501334597, 0.005846006928942671, 0.004637920789824308]  |\n",
      "|6    |[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] |[0.016965153894538152, 0.013863081708290335, 0.010508774725013163, 0.009326176148912402, 0.009303548397435113, 0.009231058548112167, 0.008379469932480343, 0.006309795792844843, 0.005666271891208685, 0.004735267463676174]|\n",
      "|7    |[0, 1, 2, 3, 4, 5, 6, 7, 8, 10]|[0.01629123674580382, 0.014059762791710076, 0.010552774849722953, 0.009697652269765225, 0.009127746861046093, 0.009104149123103993, 0.007872096591630826, 0.006207276721171024, 0.005764870554081039, 0.004756639532303077] |\n",
      "|8    |[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] |[0.016072264559367302, 0.013827094086322282, 0.010281367321409654, 0.009878332042865187, 0.009402539710636475, 0.008982872154425312, 0.00810773910904498, 0.005989888507486252, 0.005698240834768969, 0.004945161072419801] |\n",
      "|9    |[0, 1, 2, 4, 3, 5, 6, 8, 7, 9] |[0.016626901373611107, 0.01373004333555648, 0.01038240445161662, 0.009449321249513655, 0.009362995417153245, 0.00925410623160199, 0.008361904769828465, 0.0062308006277964805, 0.006174581472619309, 0.0046670730258050784] |\n",
      "+-----+-------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topics_describe_tf = lda_tf_model.describeTopics()\n",
    "topics_describe_tf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tf = topics_describe_tf.withColumn(\"Terms\", termsIdx2Term(vocabList)(\"termIndices\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------+\n",
      "|Terms                                                              |\n",
      "+-------------------------------------------------------------------+\n",
      "|[learn, ai, data, intellig, use, machin, artifici, new, read, busi]|\n",
      "|[learn, ai, data, intellig, use, machin, artifici, new, read, help]|\n",
      "|[learn, ai, data, intellig, machin, use, artifici, new, read, help]|\n",
      "|[learn, ai, data, use, intellig, machin, artifici, new, read, help]|\n",
      "|[learn, ai, data, use, machin, intellig, artifici, new, read, help]|\n",
      "|[learn, ai, data, use, machin, intellig, artifici, new, read, help]|\n",
      "|[learn, ai, data, use, intellig, machin, artifici, new, read, help]|\n",
      "|[learn, ai, data, use, intellig, machin, artifici, new, read, busi]|\n",
      "|[learn, ai, data, use, intellig, machin, artifici, new, read, help]|\n",
      "|[learn, ai, data, intellig, use, machin, artifici, read, new, help]|\n",
      "+-------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_tf.select(\"Terms\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.440127189501274"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vect_sample_check_score = df_vect.sample(fraction=0.2, seed=1234)\n",
    "lda_tf_model.logPerplexity(df_vect_sample_check_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2348721.882527943"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_tf_model.trainingLogLikelihood()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Check the topic distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df_sample = lda_mode.transform(df_sample_check_score).select(\"tweet_text\", \"topicDistribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- topicDistribution: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster_df_sample.select(\"topicDistribution\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(topicDistribution=DenseVector([0.0934, 0.0968, 0.1134, 0.104, 0.1029, 0.0864, 0.1062, 0.0917, 0.0903, 0.1149])),\n",
       " Row(topicDistribution=DenseVector([0.1621, 0.0703, 0.0672, 0.0666, 0.1206, 0.0686, 0.1824, 0.1196, 0.0763, 0.0663]))]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_df_sample.select(\"topicDistribution\").limit(2).rdd.reduce(lambda x,y: x.toArray() + y.toArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1770.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1770.0 (TID 880, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/lcx/package/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1533, in __getattr__\n    idx = self.__fields__.index(item)\nValueError: 'toArray' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/lcx/package/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/Users/lcx/package/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/lcx/package/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/lcx/package/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 842, in func\n  File \"/Users/lcx/package/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n  File \"<ipython-input-76-b951ec9e2aae>\", line 2, in <lambda>\n  File \"/Users/lcx/package/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1538, in __getattr__\n    raise AttributeError(item)\nAttributeError: toArray\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/lcx/package/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1533, in __getattr__\n    idx = self.__fields__.index(item)\nValueError: 'toArray' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/lcx/package/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/Users/lcx/package/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/lcx/package/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/lcx/package/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 842, in func\n  File \"/Users/lcx/package/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n  File \"<ipython-input-76-b951ec9e2aae>\", line 2, in <lambda>\n  File \"/Users/lcx/package/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1538, in __getattr__\n    raise AttributeError(item)\nAttributeError: toArray\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-b951ec9e2aae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moperator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcluster_df_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"topicDistribution\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/package/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    842\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 844\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/package/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/package/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/package/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/package/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1770.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1770.0 (TID 880, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/lcx/package/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1533, in __getattr__\n    idx = self.__fields__.index(item)\nValueError: 'toArray' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/lcx/package/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/Users/lcx/package/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/lcx/package/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/lcx/package/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 842, in func\n  File \"/Users/lcx/package/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n  File \"<ipython-input-76-b951ec9e2aae>\", line 2, in <lambda>\n  File \"/Users/lcx/package/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1538, in __getattr__\n    raise AttributeError(item)\nAttributeError: toArray\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/lcx/package/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1533, in __getattr__\n    idx = self.__fields__.index(item)\nValueError: 'toArray' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/lcx/package/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/Users/lcx/package/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/lcx/package/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/lcx/package/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 842, in func\n  File \"/Users/lcx/package/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n  File \"<ipython-input-76-b951ec9e2aae>\", line 2, in <lambda>\n  File \"/Users/lcx/package/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1538, in __getattr__\n    raise AttributeError(item)\nAttributeError: toArray\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "cluster_df_sample.select(\"topicDistribution\").limit(2).rdd.reduce(lambda x,y: x.toArray() + y.toArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_array = F.udf(lambda v: v.toArray().tolist(), ArrayType(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df_sample_toArray = cluster_df_sample.withColumn(\"topicDistributionArray\", \n",
    "                                                         to_array(\"topicDistribution\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|topics_Overall                                                                                                                                                                              |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[723.6276314426214, 705.2695701345801, 705.854123800993, 696.3106028027833, 714.9629479851574, 715.0801446791738, 708.341060584411, 722.5668835528195, 705.5965143069625, 701.3905211668462]|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster_df_sample_overall = cluster_df_sample_toArray.agg(\n",
    "    F.array(*[F.sum(F.col(\"topicDistributionArray\")[i]) for i in range(10)]).alias(\"topics_Overall\"))\n",
    "\n",
    "cluster_df_sample_overall.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_info = cluster_df_sample_overall.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "topics_nums = topics_info.loc[0, \"topics_Overall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['topic_0',\n",
       " 'topic_1',\n",
       " 'topic_2',\n",
       " 'topic_3',\n",
       " 'topic_4',\n",
       " 'topic_5',\n",
       " 'topic_6',\n",
       " 'topic_7',\n",
       " 'topic_8',\n",
       " 'topic_9']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\"topic_\" + str(i) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_info_final = pd.DataFrame(topics_nums, index= [\"topic_\" + str(i) for i in range(10)], columns=[\"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_info_final = topics_info_final.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>topic_0</th>\n",
       "      <td>723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_1</th>\n",
       "      <td>705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_2</th>\n",
       "      <td>705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_3</th>\n",
       "      <td>696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_4</th>\n",
       "      <td>714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_5</th>\n",
       "      <td>715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_6</th>\n",
       "      <td>708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_7</th>\n",
       "      <td>722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_8</th>\n",
       "      <td>705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_9</th>\n",
       "      <td>701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         count\n",
       "topic_0    723\n",
       "topic_1    705\n",
       "topic_2    705\n",
       "topic_3    696\n",
       "topic_4    714\n",
       "topic_5    715\n",
       "topic_6    708\n",
       "topic_7    722\n",
       "topic_8    705\n",
       "topic_9    701"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_info_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------------------+\n",
      "|          tweet_text|   topicDistribution|topicDistributionArray|\n",
      "+--------------------+--------------------+----------------------+\n",
      "|the latest thinki...|[0.09336869368277...|  [0.093368694, 0.0...|\n",
      "|weekly covid over...|[0.16212564426412...|  [0.16212565, 0.07...|\n",
      "|a pytorch re impl...|[0.07550098057259...|  [0.07550098, 0.08...|\n",
      "|beyond the hype a...|[0.06360133525585...|  [0.06360134, 0.31...|\n",
      "+--------------------+--------------------+----------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster_df_sample_toArray.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df_sample_splits = cluster_df_sample_toArray.select(\n",
    "                                                            [(F.col(\"topicDistributionArray\")[i]).alias(\"topic_\"+str(i)) for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+-----------+----------+-----------+-----------+-----------+----------+-----------+\n",
      "|    topic_0|   topic_1|    topic_2|    topic_3|   topic_4|    topic_5|    topic_6|    topic_7|   topic_8|    topic_9|\n",
      "+-----------+----------+-----------+-----------+----------+-----------+-----------+-----------+----------+-----------+\n",
      "|0.093368694|0.09681352| 0.11344034|0.103976846|0.10290956| 0.08637614| 0.10619131|  0.0917152|0.09031064| 0.11489776|\n",
      "| 0.16212565|0.07032451|0.067157984| 0.06660924|0.12064729| 0.06856954| 0.18237531| 0.11959137|0.07634403| 0.06625508|\n",
      "| 0.07550098|0.08948835| 0.18738498| 0.09091354|0.08460948|  0.0824092| 0.07731093|0.087777644|0.14683178|0.077773124|\n",
      "| 0.06360134|0.31425086| 0.06616193|0.078847855| 0.0749395|0.085080855|0.077667326| 0.06145126|0.10746074| 0.07053835|\n",
      "| 0.28822455|0.10308446|  0.0700133| 0.06900075|0.06809111|0.096229084| 0.06805478| 0.06913701|0.09985898|   0.068306|\n",
      "+-----------+----------+-----------+-----------+----------+-----------+-----------+-----------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster_df_sample_splits.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. put it into a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.clustering import LDA\n",
    "from sparknlp.annotator import Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stemmer library\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "seedNum = 1234\n",
    "\n",
    "# set parameters\n",
    "more_stopwords = [\"also\", \"via\", \"cc\", \"rt\", \"must\", \"always\"]\n",
    "\n",
    "n_topics = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def termsIdx2Term(vocabulary):\n",
    "    def termsIdx2Term(termIndices):\n",
    "        return [vocabulary[int(index)] for index in termIndices]\n",
    "    return F.udf(termsIdx2Term, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tweets:  35461\n"
     ]
    }
   ],
   "source": [
    "# 1. load file\n",
    "files_path = \"../twitter_data/sample_data/2020_10_30/*/*\"\n",
    "\n",
    "file_schema = StructType([StructField(\"tweet_text\", StringType(), True), \n",
    "                          # StructField(\"hash_tag\", ArrayType(StringType(), True), True), \n",
    "                          StructField(\"hash_tag\", StringType(), True), \n",
    "                          StructField(\"created_at\", StringType(), True), \n",
    "                          StructField(\"retweet_count\", IntegerType(), True), \n",
    "                          StructField(\"favorite_count\", IntegerType(), True), \n",
    "                          StructField(\"retweeted\", BooleanType(), True), \n",
    "                          StructField(\"truncated\", BooleanType(), True), \n",
    "                          StructField(\"id\", StringType(), True), \n",
    "                          StructField(\"user_name\", StringType(), True), \n",
    "                          StructField(\"screen_name\", StringType(), True), \n",
    "                          StructField(\"followers_count\", IntegerType(), True), \n",
    "                          StructField(\"location\", StringType(), True), \n",
    "                          StructField(\"geo\", StringType(), True),\n",
    "                          StructField(\"invalid\", StringType(), True)])\n",
    "\n",
    "\n",
    "# how to prepare the train and test dataset\n",
    "df = (spark\n",
    "      .read\n",
    "      .format(\"csv\")\n",
    "      .options(header=False, sep=\"\\t\", enforceSchema=True)\n",
    "      .schema(file_schema)\n",
    "      .load(files_path))\n",
    "\n",
    "# columns of interest\n",
    "cols_select = ['tweet_text', 'hash_tag', 'created_at', 'retweet_count', 'favorite_count']\n",
    "\n",
    "df_select = df.dropna(subset=[\"tweet_text\"]).select(cols_select)\n",
    "\n",
    "# 2. text processing\n",
    "\n",
    "df_select_clean = (df_select.withColumn(\"tweet_text\", F.regexp_replace(\"tweet_text\", r\"[@#&][A-Za-z0-9_-]+\", \" \"))\n",
    "                   .withColumn(\"tweet_text\", F.regexp_replace(\"tweet_text\", r\"\\w+:\\/\\/\\S+\", \" \"))\n",
    "                   .withColumn(\"tweet_text\", F.regexp_replace(\"tweet_text\", r\"[^A-Za-z]\", \" \"))\n",
    "                   .withColumn(\"tweet_text\", F.regexp_replace(\"tweet_text\", r\"\\s+\", \" \"))\n",
    "                   .withColumn(\"tweet_text\", F.lower(F.col(\"tweet_text\")))\n",
    "                   .withColumn(\"tweet_text\", F.trim(F.col(\"tweet_text\")))\n",
    "                  )\n",
    "\n",
    "\n",
    "df_select_clean.cache()\n",
    "print(\"total tweets: \", df_select_clean.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test = df_select_clean.randomSplit([0.8, 0.2], seed=seedNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================\n",
    "# preprocessing\n",
    "#============================================\n",
    "# 2.1. tokenize\n",
    "tokenizer = Tokenizer(inputCol=\"tweet_text\", outputCol=\"tokens\")\n",
    "\n",
    "# 2.2. remove stopwords\n",
    "stopword_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"remove_stop\")\n",
    "\n",
    "stopwords_list = stopword_remover.getStopWords()\n",
    "stopwords_list = stopwords_list + more_stopwords\n",
    "stopword_remover.setStopWords(stopwords_list)\n",
    "#2.3. stemming\n",
    "# TODO: how to modify the stemming function into a transformer?\n",
    "stemmer = PorterStemmer()\n",
    "# more straightforward to use lambda\n",
    "stem_udf = F.udf(lambda l : [stemmer.stem(word) for word in l], returnType = ArrayType(StringType()))\n",
    "# To integrate into a pipeline, using spark NLP\n",
    "# stemmer = Stemmer().setInputCols([\"remove_stop\"]).setOutputCol(\"stemmed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[tweet_text: string, hash_tag: string, created_at: string, retweet_count: int, favorite_count: int]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokenized = tokenizer.transform(df_select_clean)\n",
    "df_rmstop = stopword_remover.transform(df_tokenized)\n",
    "df_stemmed = df_rmstop.withColumn(\"stemmed\", stem_udf(F.col(\"remove_stop\")))\n",
    "\n",
    "df_stemmed.cache()\n",
    "df_select_clean.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df_stemmed.randomSplit([0.8, 0.2], seed=seedNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4. CountVectorizer\n",
    "vectorizer = CountVectorizer(inputCol= \"stemmed\", outputCol=\"rawFeatures\")\n",
    "# 2.5. IDf\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "# 3. train the LDA model\n",
    "lda = LDA(k=n_topics, seed=seedNum, optimizer=\"em\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[vectorizer, idf, lda])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelinePath = \"./LDA-pipeline-model\" \n",
    "\n",
    "pipeline_model.write().overwrite().save(pipelinePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1. get the LDA model and other useful part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "lda_model = pipeline_model.stages[-1]\n",
    "\n",
    "vectorizer_model = pipeline_model.stages[0]\n",
    "vocabList = vectorizer_model.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------+\n",
      "|Terms                                                                              |\n",
      "+-----------------------------------------------------------------------------------+\n",
      "|[base, day, cough, detect, record, recommend, asymptomat, role, work, product]     |\n",
      "|[research, great, organ, iot, talk, social, ethic, p, grow, thing]                 |\n",
      "|[program, learn, intellig, take, virtual, system, innov, financi, decis, healthcar]|\n",
      "|[news, tool, top, brain, ai, report, artifici, defeat, voic, fear]                 |\n",
      "|[ai, data, latest, join, potenti, us, read, creat, report, novemb]                 |\n",
      "|[learn, billion, challeng, differ, ai, qualiti, machin, tree, artifici, data]      |\n",
      "|[ai, scienc, get, predict, global, learn, built, price, market, give]              |\n",
      "|[check, learn, deep, machin, best, vs, one, award, help, track]                    |\n",
      "|[free, use, app, onlin, visit, look, technolog, student, sql, confer]              |\n",
      "|[book, valu, languag, c, miss, across, learn, gener, smart, data]                  |\n",
      "+-----------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the topics and the associated words\n",
    "topics_describe = lda_model.describeTopics()\n",
    "\n",
    "final = topics_describe.withColumn(\"Terms\", termsIdx2Term(vocabList)(\"termIndices\"))\n",
    "\n",
    "final.select(\"Terms\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9897287.829925386"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.trainingLogLikelihood()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogLikelihood train: -9897287.8299\n",
      "LogLikelihood test:\n"
     ]
    }
   ],
   "source": [
    "print(f\"LogLikelihood train: {lda_model.trainingLogLikelihood(): .4f}\")\n",
    "print(f\"LogLikelihood test:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7639602.470720407"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.logLikelihood(idf_model.transform(vectorizer_model.transform(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_model = pipeline_model.stages[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.532331662921242"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.logPerplexity(idf_model.transform(vectorizer_model.transform(train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.71173770845965"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.logPerplexity(idf_model.transform(vectorizer_model.transform(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. Extract more information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[F.sum(F.col(\"topic_\"+str(i) for i in range(n_topicstopics)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. check the topic distribution among dataset\n",
    "df_with_topics = pipeline_model.transform(test).select(\"tweet_text\", \"topicDistribution\")\n",
    "to_array = F.udf(lambda v: v.toArray().tolist(), ArrayType(FloatType()))\n",
    "df_with_topics_toArray = df_with_topics.select(\"tweet_text\", to_array(\"topicDistribution\").alias(\"topicDistributionArray\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_topics_toArray.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_topics_final = df_with_topics_toArray.select([\"tweet_text\"] + \n",
    "                                                     [(F.col(\"topicDistributionArray\")[i]).alias(\"topic_\"+str(i)) for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_topics_final.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|    topic_0_count|    topic_1_count|    topic_2_count|    topic_3_count|    topic_4_count|    topic_5_count|    topic_6_count|    topic_7_count|    topic_8_count|    topic_9_count|\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|710.9265619385988|695.2507085613906|719.5939190723002|703.1157946716994|680.9508484285325|710.3112846724689|692.3221873827279|693.8059616852552|680.4239798728377|705.2987543363124|\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_topics_final.agg(*[F.sum(F.col(\"topic_\"+str(i))).alias(\"topic_\"+str(i)+ \"_count\") for i in range(n_topics)]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_info_final = df_with_topics_final.agg(*[F.sum(F.col(\"topic_\"+str(i)))\n",
    "                                               .alias(\"topic_\"+str(i)+ \"_count\") \n",
    "                                               for i in range(n_topics)]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>topic_0_count</th>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_1_count</th>\n",
       "      <td>695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_2_count</th>\n",
       "      <td>719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_3_count</th>\n",
       "      <td>703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_4_count</th>\n",
       "      <td>680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_5_count</th>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_6_count</th>\n",
       "      <td>692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_7_count</th>\n",
       "      <td>693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_8_count</th>\n",
       "      <td>680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_9_count</th>\n",
       "      <td>705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0\n",
       "topic_0_count  710\n",
       "topic_1_count  695\n",
       "topic_2_count  719\n",
       "topic_3_count  703\n",
       "topic_4_count  680\n",
       "topic_5_count  710\n",
       "topic_6_count  692\n",
       "topic_7_count  693\n",
       "topic_8_count  680\n",
       "topic_9_count  705"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_info_final.astype('int32').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Load the train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', \"i'll\", \"you'll\", \"he'll\", \"she'll\", \"we'll\", \"they'll\", \"i'd\", \"you'd\", \"he'd\", \"she'd\", \"we'd\", \"they'd\", \"i'm\", \"you're\", \"he's\", \"she's\", \"it's\", \"we're\", \"they're\", \"i've\", \"we've\", \"you've\", \"they've\", \"isn't\", \"aren't\", \"wasn't\", \"weren't\", \"haven't\", \"hasn't\", \"hadn't\", \"don't\", \"doesn't\", \"didn't\", \"won't\", \"wouldn't\", \"shan't\", \"shouldn't\", \"mustn't\", \"can't\", \"couldn't\", 'cannot', 'could', \"here's\", \"how's\", \"let's\", 'ought', \"that's\", \"there's\", \"what's\", \"when's\", \"where's\", \"who's\", \"why's\", 'would', 'also', 'via', 'cc', 'rt', 'must', 'always']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187\n"
     ]
    }
   ],
   "source": [
    "print(len(stopwords_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/lcx/Documents/weclouddata/my_project/spark_ML/stopwords_list.json\", \"w\") as fp:\n",
    "    json.dump(stopwords_list, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/lcx/Documents/weclouddata/my_project/spark_ML/stopwords_list.json\", \"r\") as fp:\n",
    "    stopwords_list_load = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187\n"
     ]
    }
   ],
   "source": [
    "print(len(stopwords_list_load))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_for_stemmed = [\"learn\", \"ai\", \"machin\", \"use\", \"intellig\", \"new\", \"artifici\", \n",
    "                     'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', \n",
    "                     'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/lcx/Documents/weclouddata/my_project/spark_ML/extra_for_stemmed.json\", \"w\") as fp:\n",
    "    json.dump(extra_for_stemmed, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelinePath = \"/Users/lcx/Documents/weclouddata/my_project/spark_ML/models/LDA-pipeline-model_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "savedPipelineModel = PipelineModel.load(pipelinePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_topics = savedPipelineModel.transform(test).select(\"tweet_text\", \"topicDistribution\")\n",
    "\n",
    "to_array = F.udf(lambda v: v.toArray().tolist(), ArrayType(FloatType()))\n",
    "df_with_topics_toArray = df_with_topics.select(\"tweet_text\", to_array(\"topicDistribution\").alias(\"topicDistributionArray\"))\n",
    "\n",
    "df_with_topics_final = df_with_topics_toArray.select([\"tweet_text\"] + \n",
    "                                                 [(F.col(\"topicDistributionArray\")[i]).alias(\"topic_\"+str(i)) for i in range(10)])\n",
    "\n",
    "df_topic_info = df_with_topics_final.agg(*[F.floor(F.sum(F.col(\"topic_\"+str(i)))).alias(\"topic_\"+str(i)+ \"_count\") for i in range(n_topics)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "|topic_0_count|topic_1_count|topic_2_count|topic_3_count|topic_4_count|topic_5_count|topic_6_count|topic_7_count|topic_8_count|topic_9_count|\n",
      "+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "|          716|          698|          703|          692|          688|          709|          700|          694|          696|          697|\n",
      "+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_topic_info.cache()\n",
    "df_topic_info.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_info_count = df_topic_info.withColumn(\"count\", F.lit(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+---------------------------------+---------------------------------+---------------------------------+---------------------------------+---------------------------------+---------------------------------+---------------------------------+---------------------------------+---------------------------------+\n",
      "|count_first(topic_0_count, false)|count_first(topic_1_count, false)|count_first(topic_2_count, false)|count_first(topic_3_count, false)|count_first(topic_4_count, false)|count_first(topic_5_count, false)|count_first(topic_6_count, false)|count_first(topic_7_count, false)|count_first(topic_8_count, false)|count_first(topic_9_count, false)|\n",
      "+---------------------------------+---------------------------------+---------------------------------+---------------------------------+---------------------------------+---------------------------------+---------------------------------+---------------------------------+---------------------------------+---------------------------------+\n",
      "|                              716|                              698|                              703|                              692|                              688|                              709|                              700|                              694|                              696|                              697|\n",
      "+---------------------------------+---------------------------------+---------------------------------+---------------------------------+---------------------------------+---------------------------------+---------------------------------+---------------------------------+---------------------------------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_topic_info_count.groupby().pivot(\"count\").agg(*[F.first(F.col(\"topic_\"+str(i) + \"_count\")) for i in range(n_topics)]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+------------------------+------------------------+------------------------+------------------------+------------------------+------------------------+------------------------+------------------------+------------------------+------------------------+\n",
      "|topic_0_count|topic_1_count|topic_2_count|topic_3_count|topic_4_count|topic_5_count|topic_6_count|topic_7_count|topic_8_count|topic_9_count|count_sum(topic_0_count)|count_sum(topic_1_count)|count_sum(topic_2_count)|count_sum(topic_3_count)|count_sum(topic_4_count)|count_sum(topic_5_count)|count_sum(topic_6_count)|count_sum(topic_7_count)|count_sum(topic_8_count)|count_sum(topic_9_count)|\n",
      "+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+------------------------+------------------------+------------------------+------------------------+------------------------+------------------------+------------------------+------------------------+------------------------+------------------------+\n",
      "|          716|          698|          703|          692|          688|          709|          700|          694|          696|          697|                     716|                     698|                     703|                     692|                     688|                     709|                     700|                     694|                     696|                     697|\n",
      "+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+------------------------+------------------------+------------------------+------------------------+------------------------+------------------------+------------------------+------------------------+------------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_topic_info_count.groupby([F.col(\"topic_\"+str(i) + \"_count\") for i in range(n_topics)]).pivot(\"count\").sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-----+\n",
      "|topic_0_count|topic_1_count|topic_2_count|topic_3_count|topic_4_count|topic_5_count|topic_6_count|topic_7_count|topic_8_count|topic_9_count|count|\n",
      "+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-----+\n",
      "|          716|          698|          703|          692|          688|          709|          700|          694|          696|          697|count|\n",
      "+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_topic_info_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|  topic|count|\n",
      "+-------+-----+\n",
      "|topic_0|  716|\n",
      "|topic_1|  698|\n",
      "|topic_2|  703|\n",
      "|topic_3|  692|\n",
      "|topic_4|  688|\n",
      "|topic_5|  709|\n",
      "|topic_6|  700|\n",
      "|topic_7|  694|\n",
      "|topic_8|  696|\n",
      "|topic_9|  697|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_topic_info.selectExpr(\"stack(10, 'topic_0', topic_0_count, \\\n",
    "                               'topic_1', topic_1_count, \\\n",
    "                               'topic_2', topic_2_count, \\\n",
    "                               'topic_3', topic_3_count, \\\n",
    "                               'topic_4', topic_4_count, \\\n",
    "                               'topic_5', topic_5_count, \\\n",
    "                               'topic_6', topic_6_count, \\\n",
    "                               'topic_7', topic_7_count, \\\n",
    "                               'topic_8', topic_8_count, \\\n",
    "                               'topic_9', topic_9_count)\").withColumnRenamed(\"col0\",\"topic\").withColumnRenamed(\"col1\",\"count\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Train with pypackage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, \"/Users/lcx/Documents/weclouddata/my_project/spark_ML/sparkLDA.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, \"/Users/lcx/Documents/weclouddata/my_project/spark_ML/sparkLDA/dist/sparkLDA-0.1/sparkLDA.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path = sys.path[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/lcx/package/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip',\n",
       " '/Users/lcx/package/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip',\n",
       " '/Users/lcx/Documents/weclouddata/my_project/notebook',\n",
       " '/Users/lcx/anaconda3/envs/ML_py37/lib/python37.zip',\n",
       " '/Users/lcx/anaconda3/envs/ML_py37/lib/python3.7',\n",
       " '/Users/lcx/anaconda3/envs/ML_py37/lib/python3.7/lib-dynload',\n",
       " '',\n",
       " '/Users/lcx/anaconda3/envs/ML_py37/lib/python3.7/site-packages',\n",
       " '/Users/lcx/anaconda3/envs/ML_py37/lib/python3.7/site-packages/IPython/extensions',\n",
       " '/Users/lcx/.ipython']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparkLDA.config import n_topics, extra_for_stemmed, seedNum\n",
    "from sparkLDA.utils import show_topics, evaluate\n",
    "from sparkLDA.processing import preprocess_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparkLDA import test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparkLDA.test import hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "hello.say_hello()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sparkLDA'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-70b8b7cfbfe9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msparkLDA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mn_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_for_stemmed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseedNum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sparkLDA'"
     ]
    }
   ],
   "source": [
    "from sparkLDA.config import n_topics, extra_for_stemmed, seedNum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
