{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "#os.environ[\"PYSPARK_PYTHON\"] = \"/opt/continuum/anaconda/bin/python\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/Library/Java/JavaVirtualMachines/jdk1.8.0_221.jdk/Contents/Home/jre\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/Users/lcx/package/spark-2.4.6-bin-hadoop2.7\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create a SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"twitter ML\").getOrCreate()\n",
    "\n",
    "# get the spark context\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.25:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.6</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>twitter ML</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f81d8758dd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read data from asw folder, text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_path = \"../twitter_data/sample_data/2020_10_30/*/*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_schema = StructType([StructField(\"tweet_text\", StringType(), True), \n",
    "                          # StructField(\"hash_tag\", ArrayType(StringType(), True), True), \n",
    "                          StructField(\"hash_tag\", StringType(), True), \n",
    "                          StructField(\"created_at\", StringType(), True), \n",
    "                          StructField(\"retweet_count\", IntegerType(), True), \n",
    "                          StructField(\"favorite_count\", IntegerType(), True), \n",
    "                          StructField(\"retweeted\", BooleanType(), True), \n",
    "                          StructField(\"truncated\", BooleanType(), True), \n",
    "                          StructField(\"id\", StringType(), True), \n",
    "                          StructField(\"user_name\", StringType(), True), \n",
    "                          StructField(\"screen_name\", StringType(), True), \n",
    "                          StructField(\"followers_count\", IntegerType(), True), \n",
    "                          StructField(\"location\", StringType(), True), \n",
    "                          StructField(\"geo\", StringType(), True),\n",
    "                          StructField(\"invalid\", StringType(), True)])\n",
    "\n",
    "# file_schema = StructType([StructField(\"_c0\", StringType(), True), \n",
    "#                           StructField(\"_c1\", ArrayType(StringType(), True), True), \n",
    "#                           StructField(\"_c2\", StringType(), True), \n",
    "#                           StructField(\"_c3\", IntegerType(), True), \n",
    "#                           StructField(\"_c4\", IntegerType(), True), \n",
    "#                           StructField(\"_c5\", BooleanType(), True), \n",
    "#                           StructField(\"_c6\", BooleanType(), True), \n",
    "#                           StructField(\"_c7\", StringType(), True), \n",
    "#                           StructField(\"_c8\", StringType(), True), \n",
    "#                           StructField(\"_c9\", StringType(), True), \n",
    "#                           StructField(\"_c10\", IntegerType(), True), \n",
    "#                           StructField(\"_c11\", StringType(), True), \n",
    "#                           StructField(\"_c12\", StringType(), True),\n",
    "#                           StructField(\"_c13\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_needConversion',\n",
       " '_needSerializeAnyField',\n",
       " 'add',\n",
       " 'fieldNames',\n",
       " 'fields',\n",
       " 'fromInternal',\n",
       " 'fromJson',\n",
       " 'json',\n",
       " 'jsonValue',\n",
       " 'names',\n",
       " 'needConversion',\n",
       " 'simpleString',\n",
       " 'toInternal',\n",
       " 'typeName']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(file_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_schema.fieldNames())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(tweet_text,StringType,true),StructField(hash_tag,StringType,true),StructField(created_at,StringType,true),StructField(retweet_count,IntegerType,true),StructField(favorite_count,IntegerType,true),StructField(retweeted,BooleanType,true),StructField(truncated,BooleanType,true),StructField(id,StringType,true),StructField(user_name,StringType,true),StructField(screen_name,StringType,true),StructField(followers_count,IntegerType,true),StructField(location,StringType,true),StructField(geo,StringType,true),StructField(invalid,StringType,true)))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = (spark\n",
    "#       .read\n",
    "#       .csv(header=False, sep=\"\\t\", schema=file_schema, enforceSchema=True, \n",
    "#            path=files_path)) # don't know why this does not work\n",
    "\n",
    "df = (spark\n",
    "      .read\n",
    "      .format(\"csv\")\n",
    "      .options(header=False, sep=\"\\t\", enforceSchema=True)\n",
    "      .schema(file_schema)\n",
    "      .load(files_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-------------+--------------+---------+---------+-------------------+--------------------+---------------+---------------+--------------------+----+-------+\n",
      "|          tweet_text|            hash_tag|          created_at|retweet_count|favorite_count|retweeted|truncated|                 id|           user_name|    screen_name|followers_count|            location| geo|invalid|\n",
      "+--------------------+--------------------+--------------------+-------------+--------------+---------+---------+-------------------+--------------------+---------------+---------------+--------------------+----+-------+\n",
      "|.@mit_hst + @MITd...|Biology, COVID19,...|Fri Oct 30 20:19:...|            0|             0|    false|     true|1322271849028456448|                 HST|        mit_hst|           2449|       Cambridge, MA|None|   null|\n",
      "|Just when I am ab...|Past3amSquad, AI,...|Fri Oct 30 20:19:...|            0|             0|    false|     true|1322271849401778176|       Sydney K, PhD|sydneyfranklins|           1236|Pretoria, South A...|None|   null|\n",
      "|And a beneficial ...|     AI, FridayReads|Fri Oct 30 20:19:...|            0|             0|     true|    false|1322271852341784576|Beka \"Bexx\" Modebade|       bexxmodd|           5944|       Maryland, USA|None|   null|\n",
      "+--------------------+--------------------+--------------------+-------------+--------------+---------+---------+-------------------+--------------------+---------------+---------------+--------------------+----+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tweet_text: string (nullable = true)\n",
      " |-- hash_tag: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- retweet_count: integer (nullable = true)\n",
      " |-- favorite_count: integer (nullable = true)\n",
      " |-- retweeted: boolean (nullable = true)\n",
      " |-- truncated: boolean (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      " |-- screen_name: string (nullable = true)\n",
      " |-- followers_count: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- geo: string (nullable = true)\n",
      " |-- invalid: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(tweet_text='.@mit_hst + @MITdeptofBE prof. James Collins spoke about harnessing synthetic #Biology to develop diagnostics for #COVID19 + how his lab is using #DeepLearning  to enhance the design of such systems  @MITEECS #jclinic @MIT_IMES #MachineLearning https://t.co/2u3JwNj7CR https://t.co/EoAMQo7UHx', hash_tag='Biology, COVID19, DeepLearning, jclinic, MachineLearning', created_at='Fri Oct 30 20:19:10 +0000 2020', retweet_count=0, favorite_count=0, retweeted=False, truncated=True, id='1322271849028456448', user_name='HST', screen_name='mit_hst', followers_count=2449, location='Cambridge, MA', geo='None', invalid=None)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35463"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tweet_text', 'hash_tag', 'created_at', 'retweet_count', 'favorite_count', 'retweeted', 'truncated', 'id', 'user_name', 'screen_name', 'followers_count', 'location', 'geo', 'invalid']\n"
     ]
    }
   ],
   "source": [
    "print(file_schema.fieldNames())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns of interest\n",
    "cols_select = ['tweet_text', 'hash_tag', 'created_at', 'retweet_count', 'favorite_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-------------+--------------+\n",
      "|          tweet_text|            hash_tag|          created_at|retweet_count|favorite_count|\n",
      "+--------------------+--------------------+--------------------+-------------+--------------+\n",
      "|.@mit_hst + @MITd...|Biology, COVID19,...|Fri Oct 30 20:19:...|            0|             0|\n",
      "|Just when I am ab...|Past3amSquad, AI,...|Fri Oct 30 20:19:...|            0|             0|\n",
      "|And a beneficial ...|     AI, FridayReads|Fri Oct 30 20:19:...|            0|             0|\n",
      "|Check out new wor...|                  AI|Fri Oct 30 20:19:...|            0|             0|\n",
      "|South Korea 🇰🇷 ...|Healthcare, IoT, ...|Fri Oct 30 20:19:...|            0|             0|\n",
      "+--------------------+--------------------+--------------------+-------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_select = df.dropna(subset=[\"tweet_text\"]).select(cols_select)\n",
    "df_select.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35461"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_select.cache()\n",
    "df_select.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[tweet_text: string, hash_tag: string, created_at: string, retweet_count: int, favorite_count: int]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_select.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Basic cleaning\n",
    "1. remove mention @xxx, hashtag #xxx;\n",
    "2. remove hyperlink;\n",
    "3. remove special characters;\n",
    "4. remove any whitespace characters (change it to one)\n",
    "\n",
    "TODO: </br>\n",
    "* also fix the abbreviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_select_clean = (df_select.withColumn(\"tweet_text\", F.regexp_replace(\"tweet_text\", r\"[@#&][A-Za-z0-9_-]+\", \" \"))\n",
    "                   .withColumn(\"tweet_text\", F.regexp_replace(\"tweet_text\", r\"\\w+:\\/\\/\\S+\", \" \"))\n",
    "                   .withColumn(\"tweet_text\", F.regexp_replace(\"tweet_text\", r\"[^0-9A-Za-z \\t]\", \" \"))\n",
    "                   .withColumn(\"tweet_text\", F.regexp_replace(\"tweet_text\", r\"\\s+\", \" \"))\n",
    "                   .withColumn(\"tweet_text\", F.lower(F.col(\"tweet_text\")))\n",
    "                   .withColumn(\"tweet_text\", F.trim(F.col(\"tweet_text\")))\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-------------+--------------+\n",
      "|          tweet_text|            hash_tag|          created_at|retweet_count|favorite_count|\n",
      "+--------------------+--------------------+--------------------+-------------+--------------+\n",
      "|prof james collin...|Biology, COVID19,...|Fri Oct 30 20:19:...|            0|             0|\n",
      "|just when i am ab...|Past3amSquad, AI,...|Fri Oct 30 20:19:...|            0|             0|\n",
      "|and a beneficial ...|     AI, FridayReads|Fri Oct 30 20:19:...|            0|             0|\n",
      "|check out new wor...|                  AI|Fri Oct 30 20:19:...|            0|             0|\n",
      "|south korea bus s...|Healthcare, IoT, ...|Fri Oct 30 20:19:...|            0|             0|\n",
      "+--------------------+--------------------+--------------------+-------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_select_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[tweet_text: string, hash_tag: string, created_at: string, retweet_count: int, favorite_count: int]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_select_clean.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".@mit_hst + @MITdeptofBE prof. James Collins spoke about harnessing synthetic #Biology to develop diagnostics for #COVID19 + how his lab is using #DeepLearning  to enhance the design of such systems  @MITEECS #jclinic @MIT_IMES #MachineLearning https://t.co/2u3JwNj7CR https://t.co/EoAMQo7UHx \n",
      "\n",
      "Just when I am about to have a glass of Gin and lemonade.... #Past3amSquad #AI #MachineLearning #DeepLearning #Coding https://t.co/sAOiSemArA \n",
      "\n",
      "And a beneficial use of #AI: Using Artificial Intelligence to Identify Powerful New Antibiotic #FridayReads  https://t.co/K8mPDdPShd \n",
      "\n",
      "Check out new work on my @Behance profile: \"Community Bank / CREDIT CARD DESIGN\" https://t.co/xAF0gSaniv   #AI \n",
      "\n",
      "South Korea 🇰🇷 bus 🚌 stops check your temperature 🤒 to fight coronavirus 😷 @wef   #Healthcare #IoT #AI #DigitalTransformation #Technology #Innovation #HealthTech via @automeme #SmartCity #COVID19 cc @mvollmer1 @IrmaRaste @Ronald_vanLoon @ipfconline1  https://t.co/YyW9hmONVX \n",
      "\n",
      "ARRIA is the language of data &amp; voice reports #freetrial #nlg Reports from #excel #powerbi #alexa #qlik #resultsbi #uipath #tibco #tableau #cpa #ai #microstrategy #covid19 Join the #bi LEADERS LOVE always defeats FEAR https://t.co/szpOjPqLc8 https://t.co/X8EGwdUTjs \n",
      "\n",
      "The latest Thinking Different Daily! https://t.co/btApka9sUD Thanks to @aneeman #ai #bropenscience \n",
      "\n",
      "Thank you @_KarenHao for featuring our work in @techreview Beautifully written! Fourier neural operator gets 1000x speedup on Navier Stokes PDEs As a bonus @_KarenHao also talks about @MCHammer time :) @Caltech @kazizzad @ZongyiLiCaltech #AI #DeepLearning https://t.co/ajNyUqy58e \n",
      "\n",
      "REMINDER: Massachusetts Paid Family and Medical Leave Next Steps https://t.co/4o3equvXpj #TheLegalLowdown #VOTE #LawSuits #LawSuit #Court #Lawyer #Litigation #Lawfirm #Justice #CaseLaw #IP #SCOTUS #CorporateLaw #COVID19 #CyberLaw #Security #TechNews #AI #WhiteCase #BakerMcKenzie \n",
      "\n",
      "Labor &amp; Employment E-Note - October 2020 https://t.co/wAFrp0VfNp #TheLegalLowdown #VOTE #LawSuits #LawSuit #Court #Lawyer #Litigation #Lawfirm #Justice #CaseLaw #IP #SCOTUS #CorporateLaw #COVID19 #CyberLaw #Security #TechNews #AI #WhiteCase #BakerMcKenzie \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sample_pd_unclean = df_select.limit(10).toPandas()\n",
    "\n",
    "for i in range(10):\n",
    "    print(df_sample_pd_unclean.iloc[i, 0], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_pd_clean = df_select_clean.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prof james collins spoke about harnessing synthetic to develop diagnostics for how his lab is using to enhance the design of such systems \n",
      "\n",
      "just when i am about to have a glass of gin and lemonade \n",
      "\n",
      "and a beneficial use of using artificial intelligence to identify powerful new antibiotic \n",
      "\n",
      "check out new work on my profile community bank credit card design \n",
      "\n",
      "south korea bus stops check your temperature to fight coronavirus via cc \n",
      "\n",
      "arria is the language of data voice reports reports from join the leaders love always defeats fear \n",
      "\n",
      "the latest thinking different daily thanks to \n",
      "\n",
      "thank you for featuring our work in beautifully written fourier neural operator gets 1000x speedup on navier stokes pdes as a bonus also talks about time \n",
      "\n",
      "reminder massachusetts paid family and medical leave next steps \n",
      "\n",
      "labor employment e note october 2020 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(df_sample_pd_clean.iloc[i, 0], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35461"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_select_clean.dropna(subset=[\"tweet_text\"]).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenzier = Tokenizer(inputCol=\"tweet_text\", outputCol=\"tokens\")\n",
    "\n",
    "df_tokenized = tokenzier.transform(df_select_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-------------+--------------+--------------------+\n",
      "|          tweet_text|            hash_tag|          created_at|retweet_count|favorite_count|              tokens|\n",
      "+--------------------+--------------------+--------------------+-------------+--------------+--------------------+\n",
      "|prof james collin...|Biology, COVID19,...|Fri Oct 30 20:19:...|            0|             0|[prof, james, col...|\n",
      "|just when i am ab...|Past3amSquad, AI,...|Fri Oct 30 20:19:...|            0|             0|[just, when, i, a...|\n",
      "|and a beneficial ...|     AI, FridayReads|Fri Oct 30 20:19:...|            0|             0|[and, a, benefici...|\n",
      "|check out new wor...|                  AI|Fri Oct 30 20:19:...|            0|             0|[check, out, new,...|\n",
      "|south korea bus s...|Healthcare, IoT, ...|Fri Oct 30 20:19:...|            0|             0|[south, korea, bu...|\n",
      "+--------------------+--------------------+--------------------+-------------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tokenized.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**some quick analyses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-------------+--------------+--------------------+----------+\n",
      "|          tweet_text|            hash_tag|          created_at|retweet_count|favorite_count|              tokens|wordCounts|\n",
      "+--------------------+--------------------+--------------------+-------------+--------------+--------------------+----------+\n",
      "|prof james collin...|Biology, COVID19,...|Fri Oct 30 20:19:...|            0|             0|[prof, james, col...|        23|\n",
      "|just when i am ab...|Past3amSquad, AI,...|Fri Oct 30 20:19:...|            0|             0|[just, when, i, a...|        13|\n",
      "|and a beneficial ...|     AI, FridayReads|Fri Oct 30 20:19:...|            0|             0|[and, a, benefici...|        13|\n",
      "|check out new wor...|                  AI|Fri Oct 30 20:19:...|            0|             0|[check, out, new,...|        12|\n",
      "|south korea bus s...|Healthcare, IoT, ...|Fri Oct 30 20:19:...|            0|             0|[south, korea, bu...|        12|\n",
      "+--------------------+--------------------+--------------------+-------------+--------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_select_clean_analysis = df_tokenized.withColumn(\"wordCounts\", F.size(F.col(\"tokens\")))\n",
    "df_select_clean_analysis.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+-------------------+--------------+-----------------+------------------+--------------+-----------------+------------------+\n",
      "|  wordCounts_mean|retweet_count_mean|favorite_count_mean|wordCounts_min|retweet_count_min|favorite_count_min|wordCounts_max|retweet_count_max|favorite_count_max|\n",
      "+-----------------+------------------+-------------------+--------------+-----------------+------------------+--------------+-----------------+------------------+\n",
      "|14.99416260116748|               0.0|                0.0|             1|                0|                 0|            61|                0|                 0|\n",
      "+-----------------+------------------+-------------------+--------------+-----------------+------------------+--------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agg_col = [\"wordCounts\", \"retweet_count\", \"favorite_count\"]\n",
    "# [F.min(F.col(c)).alias(c+\"_min\") for c in agg_col]\n",
    "df_select_clean_analysis.agg(*[F.mean(F.col(c)).alias(c+\"_mean\") for c in agg_col],\n",
    "                             *[F.min(F.col(c)).alias(c+\"_min\") for c in agg_col], \n",
    "                             *[F.max(F.col(c)).alias(c+\"_max\") for c in agg_col]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------+---------------+\n",
      "|  avg(wordCounts)|min(wordCounts)|max(wordCounts)|\n",
      "+-----------------+---------------+---------------+\n",
      "|14.99416260116748|              1|             61|\n",
      "+-----------------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_select_clean_analysis.select(F.avg(F.col(\"wordCounts\")), \n",
    "                                F.min(F.col(\"wordCounts\")), \n",
    "                                F.max(F.col(\"wordCounts\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8.0, 13.0, 20.0]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_select_clean_analysis.approxQuantile(col=\"wordCounts\", \n",
    "                                        probabilities=[0.25, 0.5, 0.75], \n",
    "                                        relativeError=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Remove stop words\n",
    "TODO: extend stop words </br>\n",
    "(\"also\", \"via\", \"cc\", ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"remove_stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', \"can't\", 'cannot', 'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\", 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', \"let's\", 'me', 'more', 'most', \"mustn't\", 'my', 'myself', 'no', 'nor', 'not', 'now', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 's', 'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\", 'will', 'with', \"won't\", 'would', \"wouldn't\", 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(stopword_remover.getStopWords()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rmstop = stopword_remover.transform(df_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-------------+--------------+--------------------+--------------------+\n",
      "|          tweet_text|            hash_tag|          created_at|retweet_count|favorite_count|              tokens|         remove_stop|\n",
      "+--------------------+--------------------+--------------------+-------------+--------------+--------------------+--------------------+\n",
      "|prof james collin...|Biology, COVID19,...|Fri Oct 30 20:19:...|            0|             0|[prof, james, col...|[prof, james, col...|\n",
      "|just when i am ab...|Past3amSquad, AI,...|Fri Oct 30 20:19:...|            0|             0|[just, when, i, a...|[glass, gin, lemo...|\n",
      "|and a beneficial ...|     AI, FridayReads|Fri Oct 30 20:19:...|            0|             0|[and, a, benefici...|[beneficial, use,...|\n",
      "|check out new wor...|                  AI|Fri Oct 30 20:19:...|            0|             0|[check, out, new,...|[check, new, work...|\n",
      "|south korea bus s...|Healthcare, IoT, ...|Fri Oct 30 20:19:...|            0|             0|[south, korea, bu...|[south, korea, bu...|\n",
      "+--------------------+--------------------+--------------------+-------------+--------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rmstop.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['prof', 'james', 'collins', 'spoke', 'about', 'harnessing', 'synthetic', 'to', 'develop', 'diagnostics', 'for', 'how', 'his', 'lab', 'is', 'using', 'to', 'enhance', 'the', 'design', 'of', 'such', 'systems'] \n",
      "\n",
      "['prof', 'james', 'collins', 'spoke', 'harnessing', 'synthetic', 'develop', 'diagnostics', 'lab', 'using', 'enhance', 'design', 'systems'] \n",
      "\n",
      "['just', 'when', 'i', 'am', 'about', 'to', 'have', 'a', 'glass', 'of', 'gin', 'and', 'lemonade'] \n",
      "\n",
      "['glass', 'gin', 'lemonade'] \n",
      "\n",
      "['and', 'a', 'beneficial', 'use', 'of', 'using', 'artificial', 'intelligence', 'to', 'identify', 'powerful', 'new', 'antibiotic'] \n",
      "\n",
      "['beneficial', 'use', 'using', 'artificial', 'intelligence', 'identify', 'powerful', 'new', 'antibiotic'] \n",
      "\n",
      "['check', 'out', 'new', 'work', 'on', 'my', 'profile', 'community', 'bank', 'credit', 'card', 'design'] \n",
      "\n",
      "['check', 'new', 'work', 'profile', 'community', 'bank', 'credit', 'card', 'design'] \n",
      "\n",
      "['south', 'korea', 'bus', 'stops', 'check', 'your', 'temperature', 'to', 'fight', 'coronavirus', 'via', 'cc'] \n",
      "\n",
      "['south', 'korea', 'bus', 'stops', 'check', 'temperature', 'fight', 'coronavirus', 'via', 'cc'] \n",
      "\n",
      "['arria', 'is', 'the', 'language', 'of', 'data', 'voice', 'reports', 'reports', 'from', 'join', 'the', 'leaders', 'love', 'always', 'defeats', 'fear'] \n",
      "\n",
      "['arria', 'language', 'data', 'voice', 'reports', 'reports', 'join', 'leaders', 'love', 'always', 'defeats', 'fear'] \n",
      "\n",
      "['the', 'latest', 'thinking', 'different', 'daily', 'thanks', 'to'] \n",
      "\n",
      "['latest', 'thinking', 'different', 'daily', 'thanks'] \n",
      "\n",
      "['thank', 'you', 'for', 'featuring', 'our', 'work', 'in', 'beautifully', 'written', 'fourier', 'neural', 'operator', 'gets', '1000x', 'speedup', 'on', 'navier', 'stokes', 'pdes', 'as', 'a', 'bonus', 'also', 'talks', 'about', 'time'] \n",
      "\n",
      "['thank', 'featuring', 'work', 'beautifully', 'written', 'fourier', 'neural', 'operator', 'gets', '1000x', 'speedup', 'navier', 'stokes', 'pdes', 'bonus', 'also', 'talks', 'time'] \n",
      "\n",
      "['reminder', 'massachusetts', 'paid', 'family', 'and', 'medical', 'leave', 'next', 'steps'] \n",
      "\n",
      "['reminder', 'massachusetts', 'paid', 'family', 'medical', 'leave', 'next', 'steps'] \n",
      "\n",
      "['labor', 'employment', 'e', 'note', 'october', '2020'] \n",
      "\n",
      "['labor', 'employment', 'e', 'note', 'october', '2020'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df_sample_pd_before = df_tokenized.limit(10).toPandas()\n",
    "df_sample_pd_rmstop = df_rmstop.limit(10).toPandas()\n",
    "\n",
    "for i in range(10):\n",
    "    print(df_sample_pd_rmstop.loc[i, \"tokens\"], \"\\n\")\n",
    "    print(df_sample_pd_rmstop.loc[i, \"remove_stop\"], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tweet_text: string (nullable = true)\n",
      " |-- hash_tag: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- retweet_count: integer (nullable = true)\n",
      " |-- favorite_count: integer (nullable = true)\n",
      " |-- tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- remove_stop: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rmstop.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stemmer library\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate stemmer object\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**so pandas_udf cannot handle complex data?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(words_list):\n",
    "    return [stemmer.stem(word) for word in words_list]\n",
    "\n",
    "stem_udf = F.udf(stem, returnType = ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stemmed = df_rmstop.select(\"remove_stop\", stem_udf(F.col(\"remove_stop\")).alias(\"stemmed\")).limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['prof', 'james', 'collins', 'spoke', 'harnessing', 'synthetic', 'develop', 'diagnostics', 'lab', 'using', 'enhance', 'design', 'systems'] \n",
      "\n",
      "['prof', 'jame', 'collin', 'spoke', 'har', 'synthet', 'develop', 'diagnost', 'lab', 'use', 'enhanc', 'design', 'system'] \n",
      "\n",
      "['glass', 'gin', 'lemonade'] \n",
      "\n",
      "['glass', 'gin', 'lemonad'] \n",
      "\n",
      "['beneficial', 'use', 'using', 'artificial', 'intelligence', 'identify', 'powerful', 'new', 'antibiotic'] \n",
      "\n",
      "['benefici', 'use', 'use', 'artifici', 'intellig', 'identifi', 'power', 'new', 'antibiot'] \n",
      "\n",
      "['check', 'new', 'work', 'profile', 'community', 'bank', 'credit', 'card', 'design'] \n",
      "\n",
      "['check', 'new', 'work', 'profil', 'commun', 'bank', 'credit', 'card', 'design'] \n",
      "\n",
      "['south', 'korea', 'bus', 'stops', 'check', 'temperature', 'fight', 'coronavirus', 'via', 'cc'] \n",
      "\n",
      "['south', 'korea', 'bu', 'stop', 'check', 'temperatur', 'fight', 'coronaviru', 'via', 'cc'] \n",
      "\n",
      "['arria', 'language', 'data', 'voice', 'reports', 'reports', 'join', 'leaders', 'love', 'always', 'defeats', 'fear'] \n",
      "\n",
      "['arria', 'languag', 'data', 'voic', 'report', 'report', 'join', 'leader', 'love', 'alway', 'defeat', 'fear'] \n",
      "\n",
      "['latest', 'thinking', 'different', 'daily', 'thanks'] \n",
      "\n",
      "['latest', 'think', 'differ', 'daili', 'thank'] \n",
      "\n",
      "['thank', 'featuring', 'work', 'beautifully', 'written', 'fourier', 'neural', 'operator', 'gets', '1000x', 'speedup', 'navier', 'stokes', 'pdes', 'bonus', 'also', 'talks', 'time'] \n",
      "\n",
      "['thank', 'featur', 'work', 'beauti', 'written', 'fourier', 'neural', 'oper', 'get', '1000x', 'speedup', 'navier', 'stoke', 'pde', 'bonu', 'also', 'talk', 'time'] \n",
      "\n",
      "['reminder', 'massachusetts', 'paid', 'family', 'medical', 'leave', 'next', 'steps'] \n",
      "\n",
      "['remind', 'massachusett', 'paid', 'famili', 'medic', 'leav', 'next', 'step'] \n",
      "\n",
      "['labor', 'employment', 'e', 'note', 'october', '2020'] \n",
      "\n",
      "['labor', 'employ', 'e', 'note', 'octob', '2020'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(df_stemmed.loc[i, \"remove_stop\"], \"\\n\")\n",
    "    print(df_stemmed.loc[i, \"stemmed\"], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-------------+--------------+--------------------+--------------------+--------------------+\n",
      "|          tweet_text|            hash_tag|          created_at|retweet_count|favorite_count|              tokens|         remove_stop|             stemmed|\n",
      "+--------------------+--------------------+--------------------+-------------+--------------+--------------------+--------------------+--------------------+\n",
      "|prof james collin...|Biology, COVID19,...|Fri Oct 30 20:19:...|            0|             0|[prof, james, col...|[prof, james, col...|[prof, jame, coll...|\n",
      "|just when i am ab...|Past3amSquad, AI,...|Fri Oct 30 20:19:...|            0|             0|[just, when, i, a...|[glass, gin, lemo...|[glass, gin, lemo...|\n",
      "|and a beneficial ...|     AI, FridayReads|Fri Oct 30 20:19:...|            0|             0|[and, a, benefici...|[beneficial, use,...|[benefici, use, u...|\n",
      "|check out new wor...|                  AI|Fri Oct 30 20:19:...|            0|             0|[check, out, new,...|[check, new, work...|[check, new, work...|\n",
      "|south korea bus s...|Healthcare, IoT, ...|Fri Oct 30 20:19:...|            0|             0|[south, korea, bu...|[south, korea, bu...|[south, korea, bu...|\n",
      "+--------------------+--------------------+--------------------+-------------+--------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_stemmed = df_rmstop.withColumn(\"stemmed\", stem_udf(F.col(\"remove_stop\")))\n",
    "\n",
    "df_stemmed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35461"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stemmed.cache()\n",
    "df_stemmed.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5. More to consider\n",
    "1. use bigram\n",
    "2. filter short vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6. Encode words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.6.1. CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should tune some parameter\n",
    "vectorizer = CountVectorizer(inputCol= \"stemmed\", outputCol=\"rawFeatures\")\n",
    "vectorizer_model = vectorizer.fit(df_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vect = vectorizer_model.transform(df_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+\n",
      "|stemmed                                                                                       |rawFeatures                                                                                                          |\n",
      "+----------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+\n",
      "|[prof, jame, collin, spoke, har, synthet, develop, diagnost, lab, use, enhanc, design, system]|(11920,[4,20,42,132,385,665,956,1203,1371,1681,1683,2374,4531],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|[glass, gin, lemonad]                                                                         |(11920,[2418,6542,6640],[1.0,1.0,1.0])                                                                               |\n",
      "|[benefici, use, use, artifici, intellig, identifi, power, new, antibiot]                      |(11920,[4,5,7,8,59,191,3580,6674],[2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                 |\n",
      "|[check, new, work, profil, commun, bank, credit, card, design]                                |(11920,[8,16,55,132,261,449,1088,1256,1921],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                   |\n",
      "|[south, korea, bu, stop, check, temperatur, fight, coronaviru, via, cc]                       |(11920,[3,9,55,200,380,898,1410,1567,1734,1868],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                           |\n",
      "+----------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_vect.select(\"stemmed\", \"rawFeatures\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.6.2. TF-IDF\n",
    "**do we need IDF for the LDA model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idf_model = idf.fit(df_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idf = idf_model.transform(df_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|rawFeatures                                                                                                          |features                                                                                                                                                                                                                                                                                                    |\n",
      "+---------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|(11920,[4,20,42,132,385,665,956,1203,1371,1681,1683,2374,4531],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|(11920,[4,20,42,132,385,665,956,1203,1371,1681,1683,2374,4531],[2.540629594027734,3.528279911001965,3.7487852547660783,4.440735547092178,5.272210292540138,5.8610964627756745,6.256709274440827,6.605015968709043,6.715016863923371,7.042229775131788,7.010481076817207,7.531778000450493,8.53030683056162])|\n",
      "|(11920,[2418,6542,6640],[1.0,1.0,1.0])                                                                               |(11920,[2418,6542,6640],[7.585845221720769,9.089922618497043,9.089922618497043])                                                                                                                                                                                                                            |\n",
      "|(11920,[4,5,7,8,59,191,3580,6674],[2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                 |(11920,[4,5,7,8,59,191,3580,6674],[5.081259188055468,2.511713616065386,2.634724055156921,2.9457369843713974,3.92943156885641,4.717315205739653,8.173631886622887,9.089922618497043])                                                                                                                        |\n",
      "|(11920,[8,16,55,132,261,449,1088,1256,1921],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                   |(11920,[8,16,55,132,261,449,1088,1256,1921],[2.9457369843713974,3.4832018566839733,3.8402704239302876,4.440735547092178,4.954756061754687,5.556236053788809,6.415773969070514,6.692027345698673,7.257341154748733])                                                                                         |\n",
      "|(11920,[3,9,55,200,380,898,1410,1567,1734,1868],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                           |(11920,[3,9,55,200,380,898,1410,1567,1734,1868],[2.4185227848013464,2.9586961290139024,3.8402704239302876,4.7657899622420645,5.261281222007948,6.185757538468542,6.762644912912626,6.892698041160823,7.042229775131788,7.180380113612605])                                                                  |\n",
      "+---------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_idf.select(\"rawFeatures\", \"features\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7. Fit a LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA(k=10, seed=1, optimizer=\"em\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_mode = lda.fit(df_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.clustering.DistributedLDAModel"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lda_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|topic|termIndices                                  |termWeights                                                                                                                                                                                                                   |\n",
      "+-----+---------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0    |[4, 71, 102, 27, 10, 362, 260, 182, 2, 372]  |[0.009657542214926401, 0.007927589170209906, 0.007051729791972844, 0.006599549862990243, 0.006240483387209483, 0.006105892484747956, 0.005081569390537329, 0.005079330853887257, 0.004964629559391639, 0.0049277111266017165] |\n",
      "|1    |[41, 213, 159, 38, 64, 31, 2, 0, 116, 240]   |[0.007509772273345716, 0.007196895735569093, 0.006837418071738582, 0.006269744464172535, 0.006217359822432162, 0.006026431973605241, 0.005816982629098843, 0.005800854404247972, 0.005683547529528258, 0.005669347961754453]  |\n",
      "|2    |[95, 13, 34, 1, 2, 0, 43, 7, 27, 6]          |[0.010197015710503647, 0.009033818131336113, 0.006478102620539913, 0.006463904416454152, 0.006071519279326595, 0.005988973881814918, 0.005718988304513002, 0.005584141084137976, 0.005528845399060966, 0.00540194623597985]   |\n",
      "|3    |[0, 100, 105, 40, 25, 125, 1, 34, 75, 203]   |[0.006865286704761472, 0.00608581606509291, 0.005795998370389968, 0.005695712870296493, 0.005040001387747969, 0.005028608636830002, 0.004804454185901946, 0.004786499080654048, 0.004683957907453333, 0.004653246984878164]   |\n",
      "|4    |[33, 118, 180, 15, 42, 56, 265, 0, 9, 181]   |[0.012404674494044856, 0.009700698064111035, 0.00891804668384763, 0.00890646256573072, 0.008198736446613975, 0.006962115720681289, 0.006846830670385206, 0.006802473651766761, 0.006258938628091739, 0.005918391502101564]    |\n",
      "|5    |[145, 1, 60, 173, 12, 81, 4, 69, 56, 78]     |[0.008511900486453108, 0.008342593707758396, 0.008272431918198962, 0.008080146269031032, 0.007662318857464721, 0.007662073985702318, 0.006844847688585231, 0.006780239228728909, 0.006757858407813257, 0.006711373002302604]  |\n",
      "|6    |[58, 39, 0, 55, 3, 107, 402, 245, 76, 44]    |[0.007395086724818661, 0.007121335319556879, 0.005798790665635283, 0.005780117734872489, 0.005691200028763671, 0.005469254564609177, 0.0053022875488035704, 0.0049835471021867275, 0.004875274817288662, 0.004634098880709989]|\n",
      "|7    |[61, 135, 70, 62, 206, 212, 113, 0, 161, 177]|[0.015319664634755606, 0.009957128880745871, 0.009847029240768216, 0.009376257190824343, 0.00789537516567297, 0.007837997575306179, 0.007667546514701005, 0.007657981760015019, 0.007638645058097415, 0.00756196366014773]    |\n",
      "|8    |[48, 86, 85, 5, 84, 124, 90, 72, 168, 223]   |[0.012329157275231243, 0.011483598341527606, 0.01011342157370784, 0.009611158319738976, 0.009295757518237552, 0.008121536742250407, 0.008065054229029638, 0.007847931340316059, 0.007322839951795422, 0.006881372185679345]   |\n",
      "|9    |[29, 80, 1, 20, 89, 217, 136, 373, 166, 0]   |[0.010651582651829921, 0.009493952985315187, 0.009172239086423721, 0.006995791194950201, 0.006283667978060029, 0.006194053592082915, 0.005834583656786644, 0.0058335585486690026, 0.005821166905908045, 0.005786289703091999] |\n",
      "+-----+---------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topics_describe = lda_mode.describeTopics()\n",
    "topics_describe.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.7.1. convert the word vector to vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262144"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_model.getVocabSize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['learn',\n",
       " 'ai',\n",
       " 'data',\n",
       " 'via',\n",
       " 'use',\n",
       " 'intellig',\n",
       " 'machin',\n",
       " 'artifici',\n",
       " 'new',\n",
       " 'cc']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_model.vocabulary[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def termsIdx2Term(vocabulary):\n",
    "    def termsIdx2Term(termIndices):\n",
    "        return [vocabulary[int(index)] for index in termIndices]\n",
    "    return F.udf(termsIdx2Term, ArrayType(StringType()))\n",
    "\n",
    "vocabList = vectorizer_model.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = topics_describe.withColumn(\"Terms\", termsIdx2Term(vocabList)(\"termIndices\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------+\n",
      "|Terms                                                                     |\n",
      "+--------------------------------------------------------------------------+\n",
      "|[use, great, train, best, read, 000, high, care, data, ve]                |\n",
      "|[10, excit, 100, next, year, look, data, learn, re, novemb]               |\n",
      "|[import, technolog, world, ai, data, learn, see, artifici, best, machin]  |\n",
      "|[learn, cours, first, 1, python, mt, ai, world, creat, complet]           |\n",
      "|[compani, secur, financi, futur, system, process, effici, learn, cc, must]|\n",
      "|[role, ai, base, drive, busi, transform, use, program, process, product]  |\n",
      "|[top, predict, learn, check, via, v, websit, student, challeng, 2]        |\n",
      "|[report, leader, detect, languag, love, record, covid, learn, alway, imag]|\n",
      "|[rt, code, book, intellig, provid, smart, innov, analyt, valu, 50]        |\n",
      "|[free, tool, ai, develop, network, amaz, neural, color, avail, learn]     |\n",
      "+--------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final.select(\"Terms\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.7.2. check the model performance\n",
    "**Note:** the calculation of log Perplexity requires a large topicsMatrix() to the driver. So one should use subsample..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.142064829812528"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_mode.logPerplexity(df_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_check_score = df_idf.sample(fraction=0.2, seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.98677336119014"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_mode.logPerplexity(df_sample_check_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
